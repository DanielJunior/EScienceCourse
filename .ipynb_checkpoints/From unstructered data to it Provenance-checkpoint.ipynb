{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From unstructered data to it Provenance\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> \n",
    "<h3>EScience - PPG IC/UFF 2017.1</h3>\n",
    "\n",
    "<h4>Daniel Junior, Kid Valeriano</h4>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relembrando...\n",
    "O projeto tem como base o artigo <strong>Provenance as data mining: Combining file system metadata with content analysis</strong>.\n",
    "</hr>\n",
    "<li>A ideia é utilizar técnicas de Mineração de Dados para reconstruir a proveniência de dados não-estruturados.</li>\n",
    "<li>Os dados não-estruturados utilizados como dados de entrada para o projeto são arquivos textos contendo versões de um artigo até que o mesmo chegasse a sua edição final.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitetura\n",
    "<div width=\"100%\" height=\"100%\">\n",
    "    <img src=\"images/arquitetura.png\" height=\"400\" width=\"800\" style=\"margin: 0 auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projeto\n",
    "<ul>\n",
    "    <li>Linguagem Python</li>\n",
    "    <li>Biblioteca Sci-kit Learn</li>\n",
    "    <li>Os dados usados no experimento são do repositório <strong><a href=\"https://l.facebook.com/l.php?u=http%3A%2F%2F137.207.234.78%2Fsearch%2Fdetail%2F&h=ATMRd4R-lyI7h1CWdhxvn6UWFuIQ6Sk5lh5mt_Yl5P4cZtgMLg2Eab3JBX_fKeG0-K69PJBwnMqh8BrtHvD__C71t0gmJg-NTkGuLNtxIehOx2CwfwQx_44sEEALKLVSY6y_fg\">CiteSeerX</a></strong></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projeto\n",
    "\n",
    "Os passos a serem seguidos são:\n",
    "\n",
    "<ol>\n",
    "    <li>Dado um diretório que contenha as revisões de arquivos, devo ser capaz de recuperar o conteúdo de cada arquivo.</li>\n",
    "    <li>Representar o conteúdo do arquivo de uma forma que seja possível aplicar algoritmos de Clustering.</li>\n",
    "    <li>Executar o algoritmo para criar os clusters.</li>\n",
    "    <li>Para cada arquivo em um cluster, recuperar os metados relativos ao dados de modificação do arquivo.</li>\n",
    "    <li>Com os metadados em mãos, exportar para o PROV-Model, fazendo o armazenamento em um banco relacional (SQLite).</li>\n",
    "    <li>Ser capaz de receber um arquivo de entrada e retornar uma representação gŕafica da proveniência do mesmo.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os, time\n",
    "from stat import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "CLUSTERS_NUMBER = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 1: Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/d2/doc684.txt', 'data/d2/doc641.txt', 'data/d2/doc43.txt', 'data/d2/doc752.txt', 'data/d2/doc167.txt', 'data/d2/doc786.txt', 'data/d2/doc154.txt', 'data/d2/doc293.txt', 'data/d2/doc272.txt', 'data/d2/doc1142.txt']\n"
     ]
    }
   ],
   "source": [
    "PATH = \"data\"\n",
    "filenames = []\n",
    "docs = []\n",
    "for path, dirs, files in os.walk(PATH):\n",
    "    for filename in files:\n",
    "        fullpath = os.path.join(path, filename)\n",
    "        with open(fullpath, 'r') as f:\n",
    "            data = f.read()\n",
    "            filenames.append(fullpath)\n",
    "            docs.append(data)        \n",
    "\n",
    "print(filenames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_data = shuffle(docs, random_state=42)[int(len(docs)*0.2):]\n",
    "test_data = shuffle(docs, random_state=42)[:int(len(docs)*0.2)]\n",
    "\n",
    "train_filenames = shuffle(filenames, random_state=42)[int(len(docs)*0.2):]\n",
    "test_filenames = shuffle(filenames, random_state=42)[:int(len(docs)*0.2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 2: Representação vetorial dos dados\n",
    "\n",
    "<div width=\"100%\" height=\"100%\">\n",
    "    <img src=\"images/tfidf.png\" style=\"margin: 0 auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 2: Representação vetorial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões da matriz:\n",
      "(928, 20202)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=200000,)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "print(\"Dimensões da matriz:\")\n",
    "print(tfidf_matrix.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 3: Executar algoritmo de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters=CLUSTERS_NUMBER, init='k-means++', max_iter=100, n_init=1,verbose=False)\n",
    "km.fit(tfidf_matrix.todense())\n",
    "print(km.labels_[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 4: Recuperar os metadados dos arquivos de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: data/d2/doc725.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1426\n",
      "filename: data/d2/doc285.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5306\n",
      "filename: data/d2/doc304.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1089\n",
      "filename: data/d2/doc1090.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1363\n",
      "filename: data/d2/doc820.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2884\n",
      "filename: data/d2/doc263.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 3136\n",
      "filename: data/d1/doc489.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5039\n",
      "filename: data/d1/doc480.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1131\n",
      "filename: data/d2/doc94.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1474\n",
      "filename: data/d2/doc952.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5971\n",
      "filename: data/d2/doc159.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2274\n",
      "filename: data/d2/doc1071.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1169\n",
      "filename: data/d2/doc387.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2094\n",
      "filename: data/d2/doc843.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5122\n",
      "filename: data/d2/doc985.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1766\n",
      "filename: data/d1/doc507.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5301\n",
      "filename: data/d1/doc465.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1024\n",
      "filename: data/d2/doc745.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 857\n",
      "filename: data/d2/doc78.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1447\n",
      "filename: data/d2/doc376.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1926\n",
      "filename: data/d1/doc471.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2795\n",
      "filename: data/d2/doc241.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 854\n",
      "filename: data/d2/doc1077.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 772\n",
      "filename: data/d2/doc100.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4459\n",
      "filename: data/d2/doc805.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4183\n",
      "filename: data/d2/doc364.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1517\n",
      "filename: data/d2/doc412.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1206\n",
      "filename: data/d2/doc613.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1350\n",
      "filename: data/d2/doc128.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 922\n",
      "filename: data/d1/doc476.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2192\n",
      "filename: data/d2/doc278.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1129\n",
      "filename: data/d2/doc1052.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 915\n",
      "filename: data/d2/doc378.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1425\n",
      "filename: data/d2/doc801.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 685\n",
      "filename: data/d2/doc75.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1313\n",
      "filename: data/d1/doc455.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 8494\n",
      "filename: data/d2/doc604.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1694\n",
      "filename: data/d2/doc1054.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5008\n",
      "filename: data/d1/doc459.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 9415\n",
      "filename: data/d1/doc509.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 20441\n",
      "filename: data/d2/doc246.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 12603\n",
      "filename: data/d2/doc20.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4323\n",
      "filename: data/d2/doc1057.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4265\n",
      "filename: data/d2/doc249.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2450\n",
      "filename: data/d2/doc1159.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2283\n",
      "filename: data/d2/doc303.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 6013\n",
      "filename: data/d2/doc361.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2474\n",
      "filename: data/d2/doc669.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5268\n",
      "filename: data/d2/doc118.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1425\n",
      "filename: data/d2/doc350.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2179\n",
      "filename: data/d2/doc677.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2075\n",
      "filename: data/d2/doc931.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 473\n",
      "filename: data/d2/doc135.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1129\n",
      "filename: data/d2/doc142.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 745\n",
      "filename: data/d2/doc887.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 361\n",
      "filename: data/d2/doc267.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 934\n",
      "filename: data/d2/doc726.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 7763\n",
      "filename: data/d2/doc1153.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1521\n",
      "filename: data/d2/doc192.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 822\n",
      "filename: data/d2/doc849.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 3888\n",
      "filename: data/d2/doc1044.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 521\n",
      "filename: data/d2/doc231.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1636\n",
      "filename: data/d2/doc965.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1311\n",
      "filename: data/d2/doc54.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 11150\n",
      "filename: data/d2/doc1033.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1882\n",
      "filename: data/d2/doc994.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 866\n",
      "filename: data/d2/doc1072.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4792\n",
      "filename: data/d2/doc812.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4275\n",
      "filename: data/d2/doc859.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2837\n",
      "filename: data/d1/doc522.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1458\n",
      "filename: data/d2/doc92.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1424\n",
      "filename: data/d2/doc1027.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 10061\n",
      "filename: data/d2/doc1096.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4111\n",
      "filename: data/d2/doc822.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 851\n",
      "filename: data/d2/doc598.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4875\n",
      "filename: data/d2/doc624.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1615\n",
      "filename: data/d2/doc1112.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1451\n",
      "filename: data/d2/doc834.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 5905\n",
      "filename: data/d2/doc38.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2176\n",
      "filename: data/d2/doc358.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1448\n",
      "filename: data/d2/doc269.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1819\n",
      "filename: data/d2/doc865.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 349\n",
      "filename: data/d2/doc660.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 6079\n",
      "filename: data/d2/doc933.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2813\n",
      "filename: data/d2/doc126.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 907\n",
      "filename: data/d2/doc1095.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 954\n",
      "filename: data/d2/doc1070.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1058\n",
      "filename: data/d2/doc346.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1293\n",
      "filename: data/d2/doc339.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2241\n",
      "filename: data/d2/doc42.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 2603\n",
      "filename: data/d2/doc64.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 6990\n",
      "filename: data/d2/doc752.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 1529\n",
      "filename: data/d2/doc83.txt\n",
      "file modified: Thu May 18 11:02:46 2017\n",
      "file size: 4015\n"
     ]
    }
   ],
   "source": [
    "clusters_predict = km.predict(tfidf_vectorizer.transform(test_data))\n",
    "cluster_0_idx =  [i for i, x in enumerate(clusters_predict) if x == 0]\n",
    "filenames_cluster_0 = [test_filenames[x] for x in cluster_0_idx ]\n",
    "#print(cluster_0_idx)\n",
    "#print(filenames_cluster_0)\n",
    "\n",
    "for filename in filenames_cluster_0:\n",
    "    st = os.stat(filename)\n",
    "    print(\"filename:\", filename)\n",
    "    print(\"file modified:\", time.asctime(time.localtime(st[ST_MTIME])))\n",
    "    print(\"file size:\", st[ST_SIZE])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: israel israeli jews people edu turkish armenian armenians arab jake\n",
      "Cluster 1: edu com year baseball organization team game subject cs lines\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "    \n",
    "#km.predict(tfidf_vectorizer.transform(test_data.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,600,600i\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Inconsolata\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Titillium+Web:400,400i,700,700i\" rel=\"stylesheet\">\n",
       "<style>\n",
       ".text_cell_render {\n",
       "font-style: regular;\n",
       "font-family: 'Fira Sans', sans-serif;\n",
       "display: block;\n",
       "}\n",
       "/*font-weight: 200;*/\n",
       "/*text-align: left;\n",
       "line-height: 100%;\n",
       "display: block;\n",
       "}*/\n",
       ".text_cell_render h1 {\n",
       "/*font-size: 24pt;*/\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       "\n",
       ".reveal h1 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       ".text_cell_render h2 {\n",
       "/*font-size: 21pt;*/\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 19pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 17pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 15pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       "div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 100%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       ".reveal div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 74%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       "\n",
       ".reveal h2 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h3 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h4 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal .code_cell {\n",
       "    font-size: 83%;\n",
       "}\n",
       ".reveal code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".reveal pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".CodeMirror{\n",
       "font-family: \"Inconsolata\", monospace;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "from urllib.request import urlopen\n",
    "HTML(urlopen('https://raw.githubusercontent.com/lmarti/jupyter_custom/master/custom.include').read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
