<document id="10.1.1.1.1551"><title src="SVM HeaderParse 0.1">COMPARISON OF CLUSTERING ALGORITHMS IN SPEAKER IDENTIFICATION</title><abstract src="SVM HeaderParse 0.1">In speaker identification, we match a given (unkown) speaker to the set of known speakers in a database. The database is constructed from the speech samples of each known speaker. Feature vectors are extracted from the samples by short-term spectral analysis, and processed further by vector quantization for locating the clusters in the feature space. We study the role of the vector quantization in the speaker identification system. We compare the performance of different clustering algorithms, and the influence of the codebook size. We want to find out, which method provides the best clustering result, and whether the difference in quality contribute to improvement in recognition accuracy of the system.</abstract><keywords><keyword id="1264">Speech processing</keyword><keyword id="1265">speaker identification</keyword><keyword id="1266">vector</keyword></keywords><authors></authors><citations src="ParsCit 1.0"><citation id="29140"><authors>B P Bogert,M J R Healy</authors><title>Tukey J.W.: The quefrency alanysis of time series for echoes: cepstrum, pseudo-autocovariance, cross-cepstrum, and saphe cracking</title><venue>Proc. Symposium Time Series Analysis</venue><venType>CONFERENCE</venType><year>1963</year><pages>209--243</pages><publisher>John Wiley and Sons</publisher><pubAddress>NY</pubAddress><raw>Bogert B.P., Healy M.J.R., Tukey J.W.: The quefrency alanysis of time series for echoes: cepstrum, pseudo-autocovariance, cross-cepstrum, and saphe cracking, Proc. Symposium Time Series Analysis, John Wiley and Sons, NY, 209-243, 1963.</raw><contexts><context> we describe the procedure for computing the feature vectors from a given speech signal s(n). The most commonly used features in speaker recognition systems are the features derived from the cepstrum [1]. Furui [8] was the first who applied cepstral analysis in speaker recognition. Pre-emphasis The speech is processed by a high-emphasis filter before input to the cepstrum analysis. This is due to the</context></contexts></citation><citation id="29141"><authors>J R</authors><title>Proakis J.G., Hansen J.H.L.: Discrete-time Processing of Speech Signals</title><year>2000</year><publisher>Macmillan Publishing Company</publisher><pubAddress>New York</pubAddress><raw>Deller Jr. J.R., Proakis J.G., Hansen J.H.L.: Discrete-time Processing of Speech Signals. (New York: Macmillan Publishing Company, 2000).</raw><contexts><context>processing is the Hamming window. Smoother functions are better than rectangular window because the latter has abrupt discontinuities in its endpoints, which is undesirable for the frequency analysis [2]. Speech production modelling Speech production can be well modeled by the sourcefilter model introduced by Fant [4]. According to the model, speech waveform is a result of two independent components:</context><context>t. Basically cepstrum computation is a deconvolution operator, which decomposes the signal into its source and filter characteristics. For the details about the way the cepstrum is computed, see e.g. [2]. The result of the deconvolution is a sequence of cepstral coefficients { c 0 , c1,..., cM −1} , where M is the desired number of coefficients. Coefficient 0 c corresponds to the total energy of the </context><context>coefficients that describe fast spectral variations, i.e. the harmonic structure. In cepstral vector, lower coefficients describe the envelope structure and higher coefficients the harmonic structure [2]. 3 VECTOR QUANTIZATION OF THE FEATURE VECTORS There are two important design questions in vector quantization: the method for generating the codebook, and the size of the codebook. Next, we study kno</context></contexts></citation><citation id="29142"><title>Equitz W.H.: A new vector quantization clustering algorithm</title><venue>IEEE Trans. on Acoustics, Speech, and Signal Processing</venue><venType>JOURNAL</venType><year>1989</year><pages>1568--1575</pages><volume>37</volume><raw>Equitz W.H.: A new vector quantization clustering algorithm, IEEE Trans. on Acoustics, Speech, and Signal Processing, 37(10): 1568-1575, October 1989.</raw><contexts><context>f each cluster. We consider the following clustering algorithms: • Random: Random codebook,s• GLA: Generalized Lloyd algorithm [11], • SOM: Self-organizing maps [12], • PNN: Pairwise nearest neighbor [3], • Split: Iterative splitting technique [5], • RLS: Randomized local search [7] Random: A random codebook can be generated by selecting K random feature vectors. It serves as a point of comparison. G</context></contexts></citation><citation id="29143"><authors>G Fant</authors><title>Acoustic Theory of Speech Production</title><venue>Mouton: The Hague</venue><venType>JOURNAL</venType><year>1960</year><raw>Fant G.: Acoustic Theory of Speech Production. (Mouton: The Hague, 1960).</raw><contexts><context>t discontinuities in its endpoints, which is undesirable for the frequency analysis [2]. Speech production modelling Speech production can be well modeled by the sourcefilter model introduced by Fant [4]. According to the model, speech waveform is a result of two independent components: the source signal produced by vocal folds and the vocal tract filter which emphasizes certain frequencies of the so</context></contexts></citation><citation id="29144"><authors>P Fränti</authors><title>Kaukoranta T., Nevalainen O.: On the splitting method for vector quantization codebook generation</title><venue>Optical Engineering</venue><venType>JOURNAL</venType><year>1997</year><volume>36</volume><raw>Fränti P., Kaukoranta T., Nevalainen O.: On the splitting method for vector quantization codebook generation, Optical Engineering, 36(11): 30433051, November 1997.</raw><contexts><context>ustering algorithms: • Random: Random codebook,s• GLA: Generalized Lloyd algorithm [11], • SOM: Self-organizing maps [12], • PNN: Pairwise nearest neighbor [3], • Split: Iterative splitting technique [5], • RLS: Randomized local search [7] Random: A random codebook can be generated by selecting K random feature vectors. It serves as a point of comparison. GLA: Generalized Lloyd algorithm (also known </context></contexts></citation><citation id="29145"><authors>P Fränti,T Kaukoranta,D-F Shen</authors><title>Chang K.-S.: Fast and memory efficient implementation of the exact PNN</title><venue>IEEE Trans. on Image Processing</venue><venType>JOURNAL</venType><year>2000</year><volume>9</volume><raw>Fränti P., Kaukoranta T., Shen D.-F., Chang K.-S.: Fast and memory efficient implementation of the exact PNN, IEEE Trans. on Image Processing, 9 (5): May 2000.</raw><contexts><context>epeated until the desired size of the codebook is obtained. The code vectors to be merged are always the ones whose merge increase the distortion least. We use the fast exact PNN method introduced in [6]. Split: An opposite, top-down approach starts with a single cluster including all the feature vectors. New clusters are then created one at a time by dividing existing clusters. The splitting process</context></contexts></citation><citation id="29146"><authors>P Fränti</authors><title>Kivijärvi J.: Randomized local search algorithm for the clustering problem, Pattern Analysis and Applications</title><raw>Fränti P., Kivijärvi J.: Randomized local search algorithm for the clustering problem, Pattern Analysis and Applications. (to appear)</raw><contexts><context>m codebook,s• GLA: Generalized Lloyd algorithm [11], • SOM: Self-organizing maps [12], • PNN: Pairwise nearest neighbor [3], • Split: Iterative splitting technique [5], • RLS: Randomized local search [7] Random: A random codebook can be generated by selecting K random feature vectors. It serves as a point of comparison. GLA: Generalized Lloyd algorithm (also known as LBG) starts with an initial codeb</context></contexts></citation><citation id="29147"><authors>S Furui</authors><title>Cepstral analysis technique for automatic speaker verification</title><venue>IEEE Trans. on Acoustics, Speech and Signal Processing</venue><venType>JOURNAL</venType><year>1981</year><pages>254--272</pages><volume>29</volume><raw>Furui S.: Cepstral analysis technique for automatic speaker verification. IEEE Trans. on Acoustics, Speech and Signal Processing, 29(2): 254-272, 1981.</raw><contexts><context>e the procedure for computing the feature vectors from a given speech signal s(n). The most commonly used features in speaker recognition systems are the features derived from the cepstrum [1]. Furui [8] was the first who applied cepstral analysis in speaker recognition. Pre-emphasis The speech is processed by a high-emphasis filter before input to the cepstrum analysis. This is due to the well-known</context></contexts></citation><citation id="29148"><authors>S Furui</authors><title>Recent advances in speaker recognition</title><venue>Pattern Recognition Letters</venue><venType>JOURNAL</venType><year>1997</year><pages>859--872</pages><volume>18</volume><raw>Furui S.: Recent advances in speaker recognition. Pattern Recognition Letters, 18: 859-872, 1997.</raw><contexts><context>Speech processing, speaker identification, vector quantization, clustering. 1 INTRODUCTION Speaker recognition is a generic term used for two related problems: speaker identification and verification [9]. In the identification task the goal is to recognize the unknown speaker from a set of N known speakers. In verification, an identity claim (e.g., a username) is given to the recognizer and the goal </context></contexts></citation><citation id="29149"><authors>A Gersho,Gray</authors><title>R.M.: Vector Quantization and Signal Compression</title><year>1992</year><publisher>Kluwer Academic Publishers</publisher><pubAddress>Dordrecht</pubAddress><raw>Gersho A., Gray R.M.: Vector Quantization and Signal Compression. (Dordrecht: Kluwer Academic Publishers, 1992).</raw></citation><citation id="29150"><authors>Y Linde,A Buzo,Gray</authors><title>R.M.: An algorithm for vector quantizer design</title><venue>IEEE Trans. on Communications</venue><venType>JOURNAL</venType><year>1980</year><pages>84--95</pages><volume>28</volume><raw>Linde Y., Buzo A., Gray R.M.: An algorithm for vector quantizer design. IEEE Trans. on Communications, 28(1): 84-95, January 1980.</raw><contexts><context>tructed from the cluster representatives, which are the vector averages of each cluster. We consider the following clustering algorithms: • Random: Random codebook,s• GLA: Generalized Lloyd algorithm [11], • SOM: Self-organizing maps [12], • PNN: Pairwise nearest neighbor [3], • Split: Iterative splitting technique [5], • RLS: Randomized local search [7] Random: A random codebook can be generated by s</context></contexts></citation><citation id="29151"><authors>N M Nasrabadi</authors><title>Feng Y.: Vector quantization of images based upon the Kohonen self-organization feature maps</title><venue>Neural Networks</venue><venType>JOURNAL</venType><year>1988</year><pages>51--8</pages><volume>1</volume><raw>Nasrabadi N.M., Feng Y.: Vector quantization of images based upon the Kohonen self-organization feature maps, Neural Networks, 1: 518, 1988.</raw><contexts><context>atives, which are the vector averages of each cluster. We consider the following clustering algorithms: • Random: Random codebook,s• GLA: Generalized Lloyd algorithm [11], • SOM: Self-organizing maps [12], • PNN: Pairwise nearest neighbor [3], • Split: Iterative splitting technique [5], • RLS: Randomized local search [7] Random: A random codebook can be generated by selecting K random feature vectors.</context></contexts></citation><citation id="29152"><authors>F K Soong,A E Rosenberg</authors><title>Juang B-H., Rabiner L.R.: A vector quantization approach to speaker recognition</title><venue>AT&amp;T Technical Journal</venue><venType>JOURNAL</venType><year>1987</year><pages>14--26</pages><volume>66</volume><raw>Soong F.K., Rosenberg A.E., Juang B-H., Rabiner L.R.: A vector quantization approach to speaker recognition, AT&amp;T Technical Journal, 66: 14-26, 1987.</raw><contexts><context>u.fi Department of Computer Science, University of Joensuu, P.O.Box 111, 80101 Joensuu, FINLAND. In this work, we study the role of the vector quantization in a VQ-based speaker identification system [13]. We aim at solving this subproblem and give an answer to the question of which clustering algorithm we should use, and how large codebooks should be used. If we manage to do this, then we could fix t</context></contexts></citation></citations><fileInfo><url>http://cs.joensuu.fi/pages/tkinnu/research/pdf/ComparisonClusteringAlgsSpeakerRec.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>d5624ebb5b09238c33260f287d633eeba51ab6f4</sha1></checkSum></checkSums></fileInfo></document>