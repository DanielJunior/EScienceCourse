<document id="10.1.1.1.1564"><title src="SVM HeaderParse 0.1">Learnability and the Statistical Structure of Language: Poverty of Stimulus Arguments Revisited</title><abstract src="SVM HeaderParse 0.1">Statistical learning, and “any account which assigns a fundamental role to segmentation, categorization, analogy, and generalization ” is rejected in Chomskyan</abstract><keywords></keywords><authors><author id="4357"><name src="SVM HeaderParse 0.1">John D. Lewis</name><order>1</order></author><author id="4358"><name src="SVM HeaderParse 0.1">Jeffrey L. Elman</name><order>2</order></author></authors><citations src="ParsCit 1.0"><citation id="29407"><authors>D Angluin</authors><title>Identifying languages from stochastic examples</title><venType>TECHREPORT</venType><year>1988</year><pubAddress>New Haven, CT</pubAddress><tech>Technical Report YALEU/DCS/RR-614</tech><raw>Angluin, D. (1988). Identifying languages from stochastic examples. Technical Report YALEU/DCS/RR-614, Yale University, New Haven, CT.</raw><contexts><context>ferent from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their non-stochastic analogues require negative evidence (Gold, 1967; Horning, 1969; Angluin, 1988). Indeed, as Chomsky (1981) observed, distributional information can provide “a kind of ‘negative evidence’” in that expectations can be formed which may then be violated. And so, in at least some ca</context></contexts></citation><citation id="29408"><authors>R Aslin,J Saffran,E Newport</authors><title>Computation of conditional probability statistics by 8-month old infants</title><venue>Psychological Science</venue><venType>JOURNAL</venType><year>1998</year><volume>9</volume><raw>Aslin, R., Saffran, J., and Newport, E. (1998). Computation of conditional probability statistics by 8-month old infants. Psychological Science, 9:321–324.</raw><contexts><context>ions, and utterance types. Recent acquisition research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable f</context></contexts></citation><citation id="29409"><authors>T Cameron-Faulkner,E Lieven,M Tomasello</authors><title>A construction based analysis of child directed speech. forthcoming</title><year>2001</year><raw>Cameron-Faulkner, T., Lieven, E., and Tomasello, M. (2001). A construction based analysis of child directed speech. forthcoming.</raw><contexts><context> speech between adults is also potentially significant. Child-directed speech contains a much greater proportion of questions estimated at about one third of the child’s input (Hart and Risley, 1995; Cameron-Faulkner et al., 2001) and thus there is more of a balance between types. This may be critical in establishing the multiple roles that, e.g.auxiliaries, can take on; and also to reserve representational space for the the </context><context>use of the lesser significance to the current research issue, and in part because it is considerably more problematic to determine whether or not a prepositional phrase is within a noun phrase. But, (Cameron-Faulkner et al., 2001) analyzed a sample from this same corpus, and they report that prepositional phrases make up about 10% of all fragments, which may be indicative of their general frequency.ssuch occurrence would prod</context></contexts></citation><citation id="29410"><authors>N Chomsky</authors><title>Reflections on Language</title><year>1975</year><publisher>Pantheon Books</publisher><pubAddress>New York</pubAddress><raw>Chomsky, N. (1975). Reflections on Language. Pantheon Books, New York.</raw><contexts><context> Statistical learning, and “any account which assigns a fundamental role to segmentation, categorization, analogy, and generalization” is rejected in Chomskyan linguistics as “mistaken in principle” (Chomsky, 1975). Acquisition is viewed, rather, as a search through the set of possible grammars for natural language, guided by successive inputs; or alternatively, as a parameter setting process in which the inpu</context><context>e hypothesis may be solved by admitting the stochastic information. Thus, if UG is to account for all and only those properties of language “that can reasonably be supposed not to have been learned” (Chomsky, 1975) we must adopt a learning theory which is sensitive to the statistical properties of the input, and reassess poverty of stimulus arguments under those theoretical assumptions. This paper illustrates </context><context>gests 1) Is the man who is smoking crazy? 2) *Is the man who smoking is crazy? that “the only reasonable conclusion is that UG contains the principle that all such rules must be structure-dependent” (Chomsky, 1975) i.e. that during the course of language acquisition, children must entertain only hypotheses which respect the abstract structural organization of language, though the data may also be consistent wi</context><context>of language to attribute to UG. These assumptions must be accurate if UG is to be attributed with all and only those properties of language “that can reasonably be supposed not to have been learned” (Chomsky, 1975). An overestimate of either the learner or the input will attribute too little to UG; an underestimate will attribute properties to UG which are, in fact, learned. The objective here was to demonstra</context></contexts></citation><citation id="29411"><authors>N Chomsky</authors><title>Lectures on Government and Binding</title><year>1981</year><publisher>Foris Publishers</publisher><pubAddress>Dordrecht, Holland</pubAddress><raw>Chomsky, N. (1981). Lectures on Government and Binding. Foris Publishers, Dordrecht, Holland.</raw></citation><citation id="29412"><authors>M Christiansen,N Chater</authors><title>Toward a connectionist model of recursion in human linguistic performance</title><venue>Cognitive Science</venue><venType>JOURNAL</venType><year>1999</year><volume>23</volume><raw>Christiansen, M. and Chater, N. (1999). Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2):157–205.</raw><contexts><context> input was structured, or the network’s memory was initially limited, and developed gradually. 4 An SRN’s performance with such recursive structures has also been shown to fit well to the human data (Christiansen and Chater, 1999).sSuch networks have also been shown to go beyond the data in interesting ways. Elman (1998) and Morris et al. (2000) showed that SRNs induce abstract grammatical categories which allow both distinct</context></contexts></citation><citation id="29413"><authors>F Cowie</authors><title>What’s Within? Nativism Reconsidered</title><year>1998</year><publisher>Oxford University Press</publisher><raw>Cowie, F. (1998). What’s Within? Nativism Reconsidered. Oxford University Press.</raw><contexts><context>than just that the input available to children does not reliably contain questions like “Is the jug of milk that’s in the fridge empty?” an assumption that has been noted to be somewhat questionable (Cowie, 1998). It is apparently also assumed that the learner makes use of no form of generalization whatsoever; for as Sampson (1989) has pointed out, evidence to distinguish the two hypotheses is provided by an</context></contexts></citation><citation id="29414"><authors>S Crain</authors><title>Language acquisition in the absence of experience</title><venue>Behavioral and Brain Sciences</venue><venType>CONFERENCE</venType><year>1991</year><pages>14--597</pages><raw>Crain, S. (1991). Language acquisition in the absence of experience. Behavioral and Brain Sciences, 14:597–650.</raw><contexts><context>istical properties of the input, and reassess poverty of stimulus arguments under those theoretical assumptions. This paper illustrates this by showing that the “parade case of an innate constraint” (Crain, 1991) i.e., Chomsky’s (1975) poverty of stimulus argument that structure dependence must be a principle of UG fails to hold once stochastic information is admitted; the property of language in question is</context><context>he structure-independent hypothesis generates ungrammatical forms like (2) in place of the correct (1), children should be expected to makessuch mistakes. Since they do not (Crain and Nakayama, 1987; Crain, 1991), despite that the correct rule is supposedly more complex, Chomsky suggests 1) Is the man who is smoking crazy? 2) *Is the man who smoking is crazy? that “the only reasonable conclusion is that UG c</context></contexts></citation><citation id="29415"><authors>S Crain,M Nakayama</authors><title>Structure dependence in grammar formation. Language</title><year>1987</year><pages>63--522</pages><raw>Crain, S. and Nakayama, M. (1987). Structure dependence in grammar formation. Language, 63:522–543.</raw><contexts><context>their lives. Thus, since the structure-independent hypothesis generates ungrammatical forms like (2) in place of the correct (1), children should be expected to makessuch mistakes. Since they do not (Crain and Nakayama, 1987; Crain, 1991), despite that the correct rule is supposedly more complex, Chomsky suggests 1) Is the man who is smoking crazy? 2) *Is the man who smoking is crazy? that “the only reasonable conclusion</context></contexts></citation><citation id="29416"><authors>S Crain,R Thornton</authors><title>Investigations in Universal Grammar: A Guide to Experiment’s on the acquisition of Syntax and Semantics</title><year>1998</year><publisher>MIT Press</publisher><raw>Crain, S. and Thornton, R. (1998). Investigations in Universal Grammar: A Guide to Experiment’s on the acquisition of Syntax and Semantics. MIT Press.</raw><contexts><context>the context units unless the network’s prediction error is greater than a set threshold value.scontinuation after any NP, e.g.,‘is the boy is . . . ’. And this is an error that children make as well (Crain and Thornton, 1998). 5. Discussion Assumptions as to the nature of the input, and the ability of the learner to utilize the information therein, clearly play a critical role in determining which properties of language </context></contexts></citation><citation id="29417"><authors>J Elman</authors><title>Finding structure in time</title><venue>Cognitive Science</venue><venType>JOURNAL</venType><year>1990</year><volume>14</volume><raw>Elman, J. (1990). Finding structure in time. Cognitive Science, 14:179–211.</raw><contexts><context>e above sort of evidence, the stochastic information in data uncontroversially available to children is sufficient to allow for learning. Building on recent work with simple recurrent networks (SRNs; Elman 1990), we show that the correct generalization emerges from the statistical structure of the data. Figure 1 shows the general structure of an SRN. The network comprises a three-layer feed-forward network </context></contexts></citation><citation id="29418"><authors>J Elman</authors><title>Distributed representations, simple recurrent networks, and grammatical structure</title><venue>Machine Learning</venue><venType>CONFERENCE</venType><year>1991</year><pages>7--195</pages><raw>Elman, J. (1991). Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7:195–225.</raw><contexts><context>cted speech that appear to be important for language acquisition, and particularly for the issue at hand. Complexity increases over time which has been shown to be a determinant of learnability (e.g. Elman, 1991, 1993) and there are also arguably meaningful shifts in the distribution of types, and the limitations on forms. The increasing complexity of the child’s input is especially relevant to the problem h</context></contexts></citation><citation id="29419"><authors>J Elman</authors><title>Learning and development in neural networks: The importance of starting small</title><venue>Cognition</venue><venType>JOURNAL</venType><year>1993</year><volume>48</volume><raw>Elman, J. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48:71–99.</raw><contexts><context>ciple, i.e., ‘is the boy who is smoking is . . . ’; in fact an auxiliary is predicted as a possible 8 The SRN responsible for these results incorporates a variant of the developmental mechanism from (Elman, 1993). That version reset the context layer at increasing intervals; the version used here is similar, but does not reset the context units unless the network’s prediction error is greater than a set thre</context></contexts></citation><citation id="29420"><authors>J Elman</authors><title>Generalization, simple recurrent networks, and the emergence of structure</title><venue>Proceedings of the 20th Annual Conference of the Cognitive Science Society, Mahway, NJ. Lawrence Erlbaum Associates</venue><venType>CONFERENCE</venType><year>1998</year><editors>In Gernsbacher, M. and Derry, S., editors</editors><raw>Elman, J. (1998). Generalization, simple recurrent networks, and the emergence of structure. In Gernsbacher, M. and Derry, S., editors, Proceedings of the 20th Annual Conference of the Cognitive Science Society, Mahway, NJ. Lawrence Erlbaum Associates.</raw></citation><citation id="29421"><authors>E Gold</authors><title>Language identification in the limit</title><venue>Information and Control</venue><venType>JOURNAL</venType><year>1967</year><pages>47--4</pages><volume>10</volume><raw>Gold, E. (1967). Language identification in the limit. Information and Control, 10:447– 474.</raw><contexts><context>y is thus significantly different from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their non-stochastic analogues require negative evidence (Gold, 1967; Horning, 1969; Angluin, 1988). Indeed, as Chomsky (1981) observed, distributional information can provide “a kind of ‘negative evidence’” in that expectations can be formed which may then be violate</context></contexts></citation><citation id="29422"><authors>R Gomez,L Gerken</authors><title>Artificial grammar learning by one-year-olds leads to specific and abstract knowledge</title><venue>Cognition</venue><venType>JOURNAL</venType><year>1999</year><volume>70</volume><raw>Gomez, R. and Gerken, L. (1999). Artificial grammar learning by one-year-olds leads to specific and abstract knowledge. Cognition, 70:109–135.</raw><contexts><context>types. Recent acquisition research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable from positive examples al</context></contexts></citation><citation id="29423"><authors>B Hart,T Risley</authors><title>Meaningful Differences in the Everyday Experiences of Young Children. Paul H</title><year>1995</year><pubAddress>Brookes, Baltimore, MD</pubAddress><raw>Hart, B. and Risley, T. (1995). Meaningful Differences in the Everyday Experiences of Young Children. Paul H. Brookes, Baltimore, MD.</raw><contexts><context>ild-directed speech and speech between adults is also potentially significant. Child-directed speech contains a much greater proportion of questions estimated at about one third of the child’s input (Hart and Risley, 1995; Cameron-Faulkner et al., 2001) and thus there is more of a balance between types. This may be critical in establishing the multiple roles that, e.g.auxiliaries, can take on; and also to reserve repr</context></contexts></citation><citation id="29424"><authors>J Horning</authors><title>A study of grammatical inference</title><venType>TECHREPORT</venType><year>1969</year><pubAddress>Stanford</pubAddress><tech>PhD thesis</tech><raw>Horning, J. (1969). A study of grammatical inference. PhD thesis, Stanford.</raw><contexts><context>gnificantly different from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their non-stochastic analogues require negative evidence (Gold, 1967; Horning, 1969; Angluin, 1988). Indeed, as Chomsky (1981) observed, distributional information can provide “a kind of ‘negative evidence’” in that expectations can be formed which may then be violated. And so, in a</context></contexts></citation><citation id="29425"><authors>B MacWhinney</authors><title>The CHILDES project: Tools for analyzing talk. Third Edition. Lawrence Erlbaum Associates</title><year>2000</year><pubAddress>Mahwah, NJ</pubAddress><raw>MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. Third Edition. Lawrence Erlbaum Associates, Mahwah, NJ.</raw><contexts><context>u were speaking of in the box with the bowling pin?”, and “Where’s this little boy who’s full of smiles?”, and even “While you’re sleeping, shall I make the breakfast?” all of which are from CHILDES (MacWhinney, 2000) 1 , and presumably instances of structures which are not overly rare in child-directed speech. Indeed, Pullum and Scholz (2001) estimate that such examples make up about one percent of a typical cor</context></contexts></citation><citation id="29426"><authors>W Morris,G Cottrell,J Elman</authors><title>A connectionist simulation of the empirical acquisition of grammatical relations</title><year>2000</year><editors>In Wermter, S. and Sun, R., editors</editors><publisher>Springer Verlag</publisher><pubAddress>Heidelberg</pubAddress><raw>Morris, W., Cottrell, G., and Elman, J. (2000). A connectionist simulation of the empirical acquisition of grammatical relations. In Wermter, S. and Sun, R., editors, Hybrid Neural Systems. Springer Verlag, Heidelberg.</raw></citation><citation id="29427"><authors>E Newport,R Aslin</authors><title>Innately constrained learning: Blending old and new approaches to language acquisition</title><venue>Proceedings of the 24th Annual Boston University Conference on Language Development</venue><venType>CONFERENCE</venType><year>2000</year><editors>In Howell, S., Fish, S., and Keith-Lucas, T., editors</editors><publisher>Cascadilla Press</publisher><pubAddress>Somerville, MA</pubAddress><raw>Newport, E. and Aslin, R. (2000). Innately constrained learning: Blending old and new approaches to language acquisition. In Howell, S., Fish, S., and Keith-Lucas, T., editors, Proceedings of the 24th Annual Boston University Conference on Language Development, Somerville, MA. Cascadilla Press.</raw><contexts><context>n research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic languages may be learnable from positive examples alone, while their non-stoc</context></contexts></citation><citation id="29428"><authors>M Piatelli-Palmarini</authors><title>Language and Learning: The debate between Jean Piaget and Noam Chomsky</title><year>1980</year><publisher>Harvard University Press</publisher><pubAddress>Cambridge, MA</pubAddress><raw>Piatelli-Palmarini, M. (1980). Language and Learning: The debate between Jean Piaget and Noam Chomsky. Harvard University Press, Cambridge, MA.</raw><contexts><context> of two sorts of rules: a structure-independent rule i.e. move the first ‘is’ or the correct structure-dependent rule. Chomsky argues that since “cases that distinguish the hypotheses rarely arise,” (Piatelli-Palmarini, 1980) at least some children can be assumed not to encounter the relevant evidence for a considerable portion of their lives. Thus, since the structure-independent hypothesis generates ungrammatical forms</context></contexts></citation><citation id="29429"><authors>G Pullum,B Scholz</authors><title>Empirical assessment of stimulus poverty arguments. The Linguistic Review</title><year>2001</year><raw>Pullum, G. and Scholz, B. (2001). Empirical assessment of stimulus poverty arguments. The Linguistic Review. to appear.</raw></citation><citation id="29430"><authors>D Rohde</authors><title>Lens: The light, efficient network simulator</title><venType>TECHREPORT</venType><year>1999</year><pubAddress>Pittsburgh, PA</pubAddress><tech>Technical Report CMUCS-99-164</tech><raw>Rohde, D. (1999). Lens: The light, efficient network simulator. Technical Report CMUCS-99-164, Carnegie Mellon University, Department of Computer Science, Pittsburgh, PA.</raw><contexts><context>entence “Is the boy who is smoking crazy?” Target words appear under the network’s predictions; and the strength of the predictions is represented vertically. 5 The networks were simulated with LENS (Rohde, 1999), and trained with a fixed learning rate of 0.01, using a variation of cross entropy which assigned smaller errors for predicting incorrectly than for failure to predict. The architecture shown in Fi</context></contexts></citation><citation id="29431"><authors>J Saffran,R Aslin,E Newport</authors><title>Statistical learning by 8-month-old infants</title><venue>Science</venue><venType>JOURNAL</venType><year>1996</year><volume>274</volume><raw>Saffran, J., Aslin, R., and Newport, E. (1996). Statistical learning by 8-month-old infants. Science, (274):1926–1928.</raw><contexts><context> grammatical constructions, and utterance types. Recent acquisition research, however, has shown that children, and even infants, are sensitive to the statistical structure of their linguistic input (Saffran et al., 1996; Aslin et al., 1998; Gomez and Gerken, 1999; Newport and Aslin, 2000). The situation with respect to learnability is thus significantly different from that which has been assumed. Stochastic language</context></contexts></citation><citation id="29432"><authors>G Sampson</authors><title>Language acquisition: Growth or learning</title><venue>Philosophical Papers</venue><venType>CONFERENCE</venType><year>1989</year><pages>18--203</pages><raw>Sampson, G. (1989). Language acquisition: Growth or learning? Philosophical Papers, 18:203–240.</raw></citation></citations><fileInfo><url>http://crl.ucsd.edu/~elman/Papers/BU2001.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>ab4c1da6b9ecfa12d31b3767abb758726be92583</sha1></checkSum></checkSums></fileInfo></document>