<document id="10.1.1.1.1573"><title src="SVM HeaderParse 0.1">Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problems</title><abstract src="SVM HeaderParse 0.1">So far, boosting has been used to improve the quality of moderately accurate learning algorithms, by weighting and combining many of their weak hypotheses into a final classifier with theoretically high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt boosting properties to data reduction techniques. In this particular context, the objective was not only to improve the success rate, but also to reduce the time and space complexities due to the storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In that framework, each weak hypothesis, which is usually built and weighted from the learning set, is replaced by a single learning instance. The weight given by boosting defines in that case the relevance of the instance, and a statistical test allows one to decide whether it can be discarded without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed problems with two classes. It is the aim of the present paper to relax the class constraint, and extend our contribution to multiclass problems. Beyond data reduction, experimental results are also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting rule brings to weighted nearest neighbor classifiers. 1.</abstract><keywords></keywords><authors><author id="4373"><name src="SVM HeaderParse 0.1">Marc Sebban</name><address src="SVM HeaderParse 0.1">42023 Saint-Etienne Cedex 2, France</address><email src="SVM HeaderParse 0.1">MARC.SEBBAN@UNIV-ST-ETIENNE.FR</email><order>1</order></author><author id="4374"><name src="SVM HeaderParse 0.1">Richard Nock</name><address src="SVM HeaderParse 0.1">Indies and Guiana University; 97275 Schoelcher Cedex, Martinique, France</address><email src="SVM HeaderParse 0.1">RNOCK@MARTINIQUE.UNIV-AG.FR</email><order>2</order></author><author id="4375"><name src="SVM HeaderParse 0.1">Stéphane Lallich</name><address src="SVM HeaderParse 0.1">University of Lyon 2, 69676 Bron Cedex, France</address><email src="SVM HeaderParse 0.1">LALLICH@UNIV-LYON2.FR</email><order>3</order></author><author id="4376"><name src="SVM HeaderParse 0.1">E. Brodley</name><order>4</order></author><author id="4377"><name src="SVM HeaderParse 0.1">Andrea Danyluk</name><order>5</order></author></authors><citations src="ParsCit 1.0"><citation id="29584"><authors>Erin Allwein,E Robert</authors><title>Schapire and Yoram Singer. Reducing multiclass to binary: a unifying approach for margin classifiers</title><venue>Journal of Machine Learning Research</venue><venType>JOURNAL</venType><pages>200--0</pages><volume>1</volume><raw>Erin Allwein, Robert E. Schapire and Yoram Singer. Reducing multiclass to binary: a unifying approach for margin classifiers. Journal of Machine Learning Research, 1, 113–141, 2000.</raw></citation></citations><fileInfo><url>http://www.univ-ag.fr/~rnock/Articles/Drafts/jmlr02-snl.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>bc2c61b09474ec1f56f9e0464690af3788b66228</sha1></checkSum></checkSums></fileInfo></document>