<document id="10.1.1.1.1593"><title src="SVM HeaderParse 0.1">A Learning Approach to Improving Sentence-Level MT Evaluation</title><abstract src="SVM HeaderParse 0.1">The problem of evaluating machine translation (MT) systems is more challenging than it may first appear, as diverse translations can often be considered equally correct. The task is even more difficult when practical circumstances require that evaluation be done automatically over short texts, for instance, during incremental system development and error analysis. While several automatic metrics, such as BLEU, have been proposed and adopted for largescale MT system discrimination, they all fail to achieve satisfactory levels of correlation with human judgments at the sentence level. Here, a new class of metrics based on machine learning is introduced. A novel method involving classifying translations as machine or humanproduced rather than directly predicting numerical human judgments eliminates the need for labor-intensive user studies as a source of training data. The resulting metric, based on support vector machines, is shown to significantly improve upon current automatic metrics, increasing correlation with human judgments at the sentence level halfway toward that achieved by an independent human evaluator. 1</abstract><keywords></keywords><authors><author id="4430"><name src="SVM HeaderParse 0.1">Alex Kulesza</name><address src="SVM HeaderParse 0.1">33 Oxford St.; Cambridge, MA 02138, USA</address><email src="SVM HeaderParse 0.1">kulesza@post.harvard.edu</email><order>1</order></author><author id="4431"><name src="SVM HeaderParse 0.1">Stuart M. Shieber</name><address src="SVM HeaderParse 0.1">33 Oxford St.; Cambridge, MA 02138, USA</address><email src="SVM HeaderParse 0.1">shieber@deas.harvard.edu</email><order>2</order></author></authors><citations src="ParsCit 1.0"><citation id="29902"><authors>Ronan Collobert,Samy Bengio,Johnny Mariethoz</authors><title>Torch: a modular machine learning software library</title><venType>TECHREPORT</venType><year>2002</year><tech>Tech. Rep. IDIAP-RR 02-46, IDIAP</tech><raw>Collobert, Ronan, Samy Bengio, and Johnny Mariethoz. 2002. Torch: a modular machine learning software library. Tech. Rep. IDIAP-RR 02-46, IDIAP.</raw><contexts><context>separator is therefore used as the continuous output of the learning-based evaluator. The Torch3 machine learning library implementation of SVMs for classification is used here with Gaussian kernels (Collobert et al. 2002). 3.2 Features In order to use the general purpose SVM for classifying translations, linguistic objects must be rerepresented in numerical form with a vector of feature values. The features used here</context></contexts></citation><citation id="29903"><title>Support vector and kernel methods for learning</title><venue>ICML Tutorial</venue><venType>JOURNAL</venType><raw>Cristianini, Nello. 2001. Support vector and kernel methods for learning. ICML Tutorial.</raw></citation><citation id="29904"><authors>George Doddington</authors><title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title><venue>In Human Language Technology: Notebook Proceedings</venue><venType>CONFERENCE</venType><year>2002</year><pages>128--132</pages><pubAddress>San Diego</pubAddress><raw>Doddington, George. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Human Language Technology: Notebook Proceedings, 128–132. San Diego.</raw><contexts><context>ate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics BLEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been</context></contexts></citation><citation id="29905"><authors>George Foster,Simona Gandrabur,Cyril Goutte,Erin Fitzgerald,Alberto Sanchis,Nicola Ueffing,John Blatz,Alex Kulesza</authors><title>Confidence estimation for machine translation</title><venue>Tech. Rep., Johns Hopkins Workshop on Speech and Language Engineering</venue><venType>CONFERENCE</venType><year>2003</year><pubAddress>Baltimore, MD</pubAddress><raw>Foster, George, Simona Gandrabur, Cyril Goutte, Erin Fitzgerald, Alberto Sanchis, Nicola Ueffing, John Blatz, and Alex Kulesza. 2003. Confidence estimation for machine translation. Tech. Rep., Johns Hopkins Workshop on Speech and Language Engineering, Baltimore, MD.</raw><contexts><context>h the MT system has the most trouble and consequently leading to improvements in its performance. Furthermore, applications such as confidence estimation rely directly on local automatic assessments (Foster et al. 2003). Since short-text evaluations can be averaged into evaluations of larger texts, a meaningful correlation for short texts is always preferable. Experiments conducted at the 2003 Johns Hopkins Worksho</context><context>on Speech and Language Engineering have suggested that the currently available automatic MT evaluation metrics provide insufficient correlation when considered at the level of an individual sentence (Foster et al. 2003). The Confidence Estimation team, after collecting human judgments of 633 single-sentence hypothesis translations on a scale from 1 to 5, reports that, although inter-judge correlation is disappointi</context><context>with human evaluators; however, evidence suggests that, at least currently, MT outputs and human translations are easily distinguished—and it goes without saying that human translations are superior (Foster et al. 2003). Indeed, if MT outputs were of such quality that they could not be distinguished from human translations, evaluation would no longer be necessary; by definition such an MT system would be successful</context></contexts></citation><citation id="29906"><authors>Eduard Hovy,Margaret King</authors><title>Andrei Popescu-Belis. 2002. Principles of context-based machine translation evaluation. Machine Translation 16:1–33</title><raw>Hovy, Eduard, Margaret King, and Andrei Popescu-Belis. 2002. Principles of context-based machine translation evaluation. Machine Translation 16:1–33.</raw></citation><citation id="29907"><title>Minimum error rate training for statistical machine translation</title><venue>In Acl 2003: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</venue><venType>CONFERENCE</venType><pubAddress>Sapporo, Japan</pubAddress><raw>Och, Franz Josef. 2003. Minimum error rate training for statistical machine translation. In Acl 2003: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. Sapporo, Japan.</raw></citation><citation id="29908"><authors>Franz Josef Och,Daniel Gildea,Sanjeev Khudanpur,Anoop Sarkar,Kenji Yamada,Alex Fraser,Shankar Kumar,Libin Shen,David Smith,Katherine Eng,Viren Jain,Zhen Jin,Dragomir Radev</authors><title>Syntax for statistical machine translation</title><venue>Tech. Rep., Johns Hopkins Workshop on Speech and Language Engineering</venue><venType>CONFERENCE</venType><year>2003</year><pubAddress>Baltimore, MD</pubAddress><raw>Och, Franz Josef, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syntax for statistical machine translation. Tech. Rep., Johns Hopkins Workshop on Speech and Language Engineering, Baltimore, MD.</raw><contexts><context>ts final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems insensitive to syntactic changes that should be noticeable to human judges at the sentence level (Och et al. 2003). Here, a metric designed to evaluate single-sentence machine translations based on machineslearning is described and tested. A novel approach leads to a flexible classification-based metric and elim</context><context>concern, thereby improving the metric’s sensitivity to particular aspects of machine translations. For example, it has been observed that BLEU is not particularly sensitive to syntactic improvements (Och et al. 2003); a learning-based metric, on the other hand, could be modified specifically to take syntax into account, providing a much finer-grained error analysis than is currently possible. 6 Acknowledgements </context></contexts></citation><citation id="29909"><authors>Kishore A Papineni,Salim Roukos,Todd Ward,Wei-Jing Zhu</authors><title>BLEU: A method for automatic evaluation of machine translation</title><venue>Tech. Rep. RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research</venue><venType>JOURNAL</venType><year>2001</year><pubAddress>Center, Yorktown Heights, NY</pubAddress><raw>Papineni, Kishore A., Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Tech. Rep. RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY.</raw><contexts><context>osition-independent word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics BLEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the fraction of output n-grams that also appear in a set of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT resu</context><context>uccess; the BLEU metric, for example, has been shown to correlate highly with the judgments of bilingual human evaluators (a correlation coefficient of 0.96) when averaged over a 500-sentence corpus (Papineni et al. 2001). However, though useful for system discrimination, metrics correlating with human evaluations only over long texts (which tend to average out the “noise” of evaluation) are relatively useless for pu</context></contexts></citation><citation id="29910"><authors>C Tillman,S Vogel,H Ney,H Sawaf,A Zubiaga</authors><title>Accelerated DP-based search for statistical translation</title><venue>In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech</venue><venType>CONFERENCE</venType><year>1997</year><pages>2667--2670</pages><pubAddress>Rhodes, Greece</pubAddress><volume>97</volume><raw>Tillman, C., S. Vogel, H. Ney, H. Sawaf, and A. Zubiaga. 1997. Accelerated DP-based search for statistical translation. In Proceedings of the 5th European Conference on Speech Communication and Technology (EuroSpeech ’97), 2667–2670. Rhodes, Greece.</raw><contexts><context>automatic metrics have been proposed and used for evaluating the overall performance of MT systems. General-purpose measures like word error rate (WER) and position-independent word error rate (PER) (Tillman et al. 1997) rely on direct correspondence between the machine translation and a single human-produced reference. The more specialized metrics BLEU (Papineni et al. 2001) and NIST (Doddington 2002) consider the </context></contexts></citation><citation id="29911"><authors>Joseph P Turian,Luke Shen,I Dan Melamed</authors><title>Evaluation of machine translation and its evaluation</title><venue>In Proceedings of MT Summit IX</venue><venType>CONFERENCE</venType><year>2003</year><pages>23--28</pages><pubAddress>New Orleans</pubAddress><raw>Turian, Joseph P., Luke Shen, and I. Dan Melamed. 2003. Evaluation of machine translation and its evaluation. In Proceedings of MT Summit IX, 23–28. New Orleans.</raw><contexts><context>t of human translations (n-gram precision), thereby acknowledging a greater diversity of acceptable MT results. The F-Measure has been proposed as a more comprehensible alternative for MT evaluation (Turian et al. 2003), and can be defined as a simple composite of unigram precision and recall. These metrics have demonstrated some success; the BLEU metric, for example, has been shown to correlate highly with the jud</context><context>though human evaluation of MT is itself inconsistent and not very reliable, automatic MT evaluation measures are even less reliable and are still very far from being able to replace human judgment.” (Turian et al. 2003) The Syntax for Statistical Machine Translation team from the 2003 Workshop further notes in its final report that the BLEU metric, used for training and evaluation of the team’s MT system, seems ins</context></contexts></citation></citations><fileInfo><url>http://www.eecs.harvard.edu/~shieber/Biblio/./Papers/kulesza-mt-evaluation04.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>08395ba1176be9b5d2887e98944f88c84f87934e</sha1></checkSum></checkSums></fileInfo></document>