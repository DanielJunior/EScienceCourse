<document id="10.1.1.1.1529"><title src="SVM HeaderParse 0.1">Discriminant Projections Embedding for Nearest Neighbor Classification.</title><abstract src="SVM HeaderParse 0.1">Abstract. In this paper we introduce a new embedding technique to linearly project labeled data samples into a new space where the performance of a Nearest Neighbor classifier is improved. The approach is based on considering a large set of simple discriminant projections and finding the subset with higher classification performance. In order to implement the feature selection process we propose the use of the adaboost algorithm. The performance of this technique is tested in a multiclass classification problem related to the production of cork stoppers for wine bottles. 1</abstract><keywords></keywords><authors><author id="4251"><name src="SVM HeaderParse 0.1">Petia Radeva</name><address src="SVM HeaderParse 0.1">08193 Bellaterra (Barcelona), Spain</address><email src="SVM HeaderParse 0.1">petia@cvc.uab.es</email><order>1</order></author><author id="4252"><name src="SVM HeaderParse 0.1">Jordi Vitrià</name><address src="SVM HeaderParse 0.1">08193 Bellaterra (Barcelona), Spain</address><email src="SVM HeaderParse 0.1">jordi@cvc.uab.es</email><order>2</order></author></authors><citations src="ParsCit 1.0"><citation id="28725"><authors>R Fisher</authors><title>On subharmonic solutions of a Hamiltonian system. The use of multiple measurements in taxonomic problems, Ann. Eugenics 7</title><year>1936</year><pages>179--188</pages><raw>R. Fisher: On subharmonic solutions of a Hamiltonian system. The use of multiple measurements in taxonomic problems, Ann. Eugenics 7 (1936) 179–188.</raw><contexts><context>Discriminant Analysis The most widely spread approach for discriminant analysis is the one that makes use of only up to second order statistics of the data. This was done in a classic paper by Fisher [1], and it is called Fisher Discriminant Analysis (FDA). In FDA the within class scatter matrix is usually computed as a weighted sum of the class-conditional sample covariance matrices where the weight</context><context>ses. 1.3 Nonparametric Discriminant Analysis In [3] Fukunaga and Mantock present a linear and nonparametric method for discriminant analysis in an attempt to overcome the limitations present in (FDA) [1], and name the technique Nonparametric Discriminant Analysis (NDA). In NDA we define a between-class matrix as the scatter matrix obtained from vectors locally pointing to another class. This is done </context></contexts></citation><citation id="28726"><authors>M Aladjem</authors><title>Linear discriminant analysis for two classes via removal of classification structure</title><venue>IEEE Trans. Pattern Anal. Machine Intell</venue><venType>JOURNAL</venType><year>1997</year><pages>187--192</pages><volume>19</volume><raw>M. Aladjem: Linear discriminant analysis for two classes via removal of classification structure, IEEE Trans. Pattern Anal. Machine Intell. 19 (2) (1997) 187–192.</raw><contexts><context>s specially designed to increase the performance of the nearest neighbor classification rule. We have not made assumptions on the data distribution, and we don’t force our projection to be orthogonal [2]. The only assumption we impose is that our embedding must be based on a set of simple 1D projections, which can complement each other to achieve better classification results. We have made use of Ada</context></contexts></citation><citation id="28727"><authors>K Fukunaga,J</authors><title>Mantock: Nonparametric discriminant analysis</title><venue>IEEE Trans. Pattern Anal. Machine Intell</venue><venType>JOURNAL</venType><year>1983</year><pages>671--678</pages><volume>5</volume><raw>K. Fukunaga, J. Mantock: Nonparametric discriminant analysis, IEEE Trans. Pattern Anal. Machine Intell. 5 (6) (1983) 671–678. 11</raw><contexts><context>: Gaussian assumption over the class distribution of the data samples; and the dimensionality of the subspaces obtained is limited by the number of classes. 1.3 Nonparametric Discriminant Analysis In [3] Fukunaga and Mantock present a linear and nonparametric method for discriminant analysis in an attempt to overcome the limitations present in (FDA) [1], and name the technique Nonparametric Discrimin</context></contexts></citation><citation id="28728"><authors>P Devijver,J Kittler</authors><title>Pattern Recognition: A Statistical Approach</title><year>1982</year><publisher>Prentice Hall</publisher><pubAddress>London, UK</pubAddress><raw>P. Devijver, J. Kittler: Pattern Recognition: A Statistical Approach, Prentice Hall, London, UK, 1982.</raw><contexts><context> done as follows: Given a norm �� in the metric space where the samples are defined, the extraclass nearest neighbor for a sample x ∈ Ck is defined as x E = {x ′ ∈ C k /�x ′ − x� ≤�z − x�, ∀z ∈ C k } (4)swhere C k notes the complement set of C k . In the same fashion we can define the intraclass nearest neighbor as x I = {x ′ ∈ C k /�x ′ − x� ≤�z − x�, ∀z ∈ C k } (5) Both definitions (4) and (5) can </context></contexts></citation></citations><fileInfo><url>http://www.cvc.uab.es/~jordi/ciarp2004.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>bb2290a63879c68477cbdc3967ca3bf298a1cdf9</sha1></checkSum></checkSums></fileInfo></document>