<document id="10.1.1.1.1552"><title src="SVM HeaderParse 0.1"></title><abstract src="SVM HeaderParse 0.1">We present here a framework for developing a generic talking head capable of reproducing the anatomy and the facial deformations induced by speech movements with a set of a few parameters. We will show that the speaker-specific articulatory movements can be straightforward encoded into the normalized</abstract><keywords><keyword id="1267">MPEG-4 Facial Animation Parameters and Facial Definition Parameters</keyword></keywords><authors></authors><citations src="ParsCit 1.0"><citation id="29153"><authors>P Badin,G Bailly,L Revéret,M Baciu,C Segebarth,C Savariaux</authors><title>Threedimentional linear articulatory modeling of tongue, lips and face based on MRI and video images</title><venue>Journal of Phonetics</venue><venType>JOURNAL</venType><year>2002</year><pages>533--553</pages><volume>30</volume><raw>Badin, P., Bailly, G., Revéret, L., Baciu, M., Segebarth, C., and Savariaux, C. (2002) Threedimentional linear articulatory modeling of tongue, lips and face based on MRI and video images. Journal of Phonetics, 30(3): p. 533-553.</raw></citation><citation id="29154"><authors>C Bregler,M Covell,M Slaney</authors><title>Video rewrite: visual speech synthesis from video</title><venue>in International Conference on Auditory-Visual Speech Processing. Rhodes</venue><venType>CONFERENCE</venType><year>1997</year><pages>153--156</pages><pubAddress>Greece</pubAddress><raw>Bregler, C., Covell, M., and Slaney, M. (1997) Video rewrite: visual speech synthesis from video. in International Conference on Auditory-Visual Speech Processing. Rhodes, Greece. p. 153-156.</raw><contexts><context>ng a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting [2, 7] or 3D animation models [8, 14], all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time a</context></contexts></citation><citation id="29155"><authors>T F Cootes,G J Edwards,C J Taylor</authors><title>Active Appearance Models</title><venue>IEEE Transactions on Pattern Analysis and Machine Intelligence</venue><venType>JOURNAL</venType><year>2001</year><pages>681--685</pages><volume>23</volume><raw>Cootes, T.F., Edwards, G.J., and Taylor, C.J. (2001) Active Appearance Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6): p. 681-685.</raw><contexts><context>cal analysis of motion capture data. Figure 3: Shape and appearance changes associated with extreme variations along the first lip component (rounding/spreading) for two speakers. Shape-free textures [3] have been obtained from image data with colored beads. (a) (b) (c) Figure 4: Rendering and recovering 3D articulation. Figure 5: Subdivision of n elementary volume of the original space and new trans</context><context> all configurations used for estimating the shape model (by warping all images to the neutral configuration). Instead of combining a posteriori separate shape and appearance models as in Cootes et al [3], we estimate a simple multilinear model that relates RGB colors of each pixel of the shape-free images to shape parameters. Figure 3 illustrates the change of shape-free appearance accompanying the r</context></contexts></citation><citation id="29156"><authors>B Couteau,Y Payan,S Lavallée</authors><title>The Mesh-Matching algorithm : an automatic 3D mesh generator for finite element structures</title><venue>Journal of Biomechanics</venue><venType>JOURNAL</venType><year>2000</year><pages>1005--1009</pages><volume>33</volume><raw>Couteau, B., Payan, Y., and Lavallée, S. (2000) The Mesh-Matching algorithm : an automatic 3D mesh generator for finite element structures. Journal of Biomechanics, 33(8): p. 1005-1009.</raw><contexts><context>. [u]), one rounded viseme with open lips (e.g. [ a� a]), one spread viseme with open lips (e.g. [i]). (a) (b) Figure 6: Building a generic talking face. Using an original 3D to 3D matching algorithm [4], a generic “high definition” but static face mesh (see Figure 1.b) is scaled to multiple “low definition” motion capture data from each speaker. A “high definition” articulated clone for each speaker</context><context>of vertices. Moreover the number of fleshpoints recorded during a motion-capture session is limited to a few hundred and do not entirely cover the whole head. Using a modified mesh-matching algorithm [4], we are able to scale a generic high-definition talking face to the low-resolution surface defined by the fleshpoints characterizing each viseme of a session (see Figure 1.b&amp;c &amp; Figure 8). 3 SHAPING </context><context>ure 1.a, 30 control points of the lips model and 29 markers for the skull as shown Figure 4.c. 3.1 3D-to-3D matching The basic principle of the 3D-to-3D matching procedure developed by Lavallée et al [4] consists basically in deforming the initial 3D space by a series of trilinear transformations applied to elementary cubes (see also Figure 7): ( ) [ ] T ⎡ p00 p01 p07 ⎤ Tl qi , p = ⎢ p p ... p ⎥ 10 1</context></contexts></citation><citation id="29157"><authors>P Eisert,B Girod</authors><title>Analyzing Facial Expressions for Virtual Conferencing</title><venue>IEEE Computer Graphics &amp; Applications: Special Issue: Computer Animation for Virtual Humans</venue><venType>JOURNAL</venType><year>1998</year><pages>70--78</pages><volume>18</volume><raw>Eisert, P. and Girod, B. (1998) Analyzing Facial Expressions for Virtual Conferencing. IEEE Computer Graphics &amp; Applications: Special Issue: Computer Animation for Virtual Humans, 18(5): p. 70-78.</raw><contexts><context>ker variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision [5, 14] or MPEG-4/SNHC coding scheme [12] require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape model</context></contexts></citation><citation id="29158"><authors>F Elisei,M Odisio,G Bailly,P Badin</authors><title>Creating and controlling video-realistic talking heads</title><venue>in Auditory-Visual Speech Processing Workshop. Scheelsminde</venue><venType>CONFERENCE</venType><year>2001</year><pages>90--97</pages><pubAddress>Denmark</pubAddress><raw>Elisei, F., Odisio, M., Bailly, G., and Badin, P. (2001) Creating and controlling video-realistic talking heads. in Auditory-Visual Speech Processing Workshop. Scheelsminde, Denmark. p. 90-97.</raw></citation><citation id="29159"><authors>T Ezzat,G Geiger,T Poggio</authors><title>Trainable videorealistic speech animation</title><venue>ACM Transactions on Graphics</venue><venType>JOURNAL</venType><year>2002</year><pages>388--398</pages><volume>21</volume><raw>Ezzat, T., Geiger, G., and Poggio, T. (2002) Trainable videorealistic speech animation. ACM Transactions on Graphics, 21(3): p. 388-398.</raw><contexts><context>ng a static generic model to speaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting [2, 7] or 3D animation models [8, 14], all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time a</context></contexts></citation><citation id="29160"><authors>B Guenter,C Grimm,D Wood,H Malvar,F Pighin</authors><title>Making faces</title><venue>in SIGGRAPH. Orlando - USA</venue><venType>CONFERENCE</venType><year>1998</year><pages>55--67</pages><raw>Guenter, B., Grimm, C., Wood, D., Malvar, H., and Pighin, F. (1998) Making faces. in SIGGRAPH. Orlando - USA. p. 55-67.</raw><contexts><context>peaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting [2, 7] or 3D animation models [8, 14], all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision the</context></contexts></citation><citation id="29161"><authors>R A Harshman,M E Lundy</authors><title>The PARAFAC model for three-way factor analysis and multidimensional scaling, in Research Methods for Multimode Data Analysis</title><year>1984</year><pages>122--215</pages><raw>Harshman, R.A. and Lundy, M.E. (1984) The PARAFAC model for three-way factor analysis and multidimensional scaling, in Research Methods for Multimode Data Analysis, H.G. Law, et al., Editors. Praeger: New-York. p. 122-215.</raw><contexts><context>e we share the facial musculo-skelettal structure; i.e. speakers and languages “just” differ in the way they exploit and synchronize these “same” elementary gestures.sWe can thus use PARAFAC analysis [9] or more directly multilinear regression to determine the speaker’s specific scaling of these universal commands. Prior to this analysis, each speaker-specific shape model should be characterized not </context></contexts></citation><citation id="29162"><authors>M Hashi,J R Westbury,K Honda</authors><title>Vowel posture normalization</title><venue>Journal of the Acoustical Society of America</venue><venType>JOURNAL</venType><year>1998</year><pages>2426--2437</pages><volume>104</volume><raw>Hashi, M., Westbury, J.R., and Honda, K. (1998) Vowel posture normalization. Journal of the Acoustical Society of America, 104(4): p. 2426-2437.</raw><contexts><context> if we share the same underlying anatomical structures, speakers differ in the way they recruit and coordinate speech organs. Part of this variability is effectively due to the anatomical differences [10] but also to different control strategies exploiting articulatory degrees-of-freedom in excess. Besides understanding interspeaker variability of articulation, there is also a clear technological need</context></contexts></citation><citation id="29163"><authors>J Ostermann,M Beutnagel,A Fischer,Y Wang</authors><title>Integration of talking heads and text-tospeech synthesizers for visual TTS</title><venue>in International Conference on Speech and Language Processing. Sydney - Australia</venue><venType>CONFERENCE</venType><year>1998</year><pages>297--300</pages><raw>Ostermann, J., Beutnagel, M., Fischer, A., and Wang, Y. (1998) Integration of talking heads and text-tospeech synthesizers for visual TTS. in International Conference on Speech and Language Processing. Sydney - Australia. p. 297-300.</raw><contexts><context>ometry of the face with no implicit reference to any articulatory model (e.g. FAP3 open_jaw “does not affect mouth opening” [19, p.412]). FAP ease however specifying constrictions sizes and positions [11] supposed to be less speaker-dependent than articulatory parameters. On the contrary articulatory models are often used to specify how constrictions sizes and positions are reached by speaker-specific</context></contexts></citation><citation id="29164"><authors>I S Pandzic,R Forchheimer</authors><title>MPEG-4 facial animation. The standard, implementation and applications</title><year>2002</year><publisher>John Wiley &amp; Sons</publisher><pubAddress>Chichester, England</pubAddress><raw>Pandzic, I.S. and Forchheimer, R. (2002) MPEG-4 facial animation. The standard, implementation and applications. Chichester, England: John Wiley &amp; Sons.</raw><contexts><context>e is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision [5, 14] or MPEG-4/SNHC coding scheme [12] require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape models by adapting a static generic mod</context></contexts></citation><citation id="29165"><authors>F Pighin,J Hecker,D Lischinski,R Szeliski,D H Salesin</authors><title>Synthesizing Realistic Facial Expressions from Photographs</title><venue>in Proceedings of Siggraph</venue><venType>CONFERENCE</venType><year>1998</year><pages>75--84</pages><pubAddress>Orlando, FL, USA</pubAddress><raw>Pighin, F., Hecker, J., Lischinski, D., Szeliski, R., and Salesin, D.H. (1998) Synthesizing Realistic Facial Expressions from Photographs. in Proceedings of Siggraph. Orlando, FL, USA. p. 75-84.</raw><contexts><context>quired in all allophonic variations of [s] for carrying the tongue front and upwards . (a) (b) (c) Figure 1: (a) Gathering fleshpoint positions; (b) The generic facial mesh developed by Pighin et al. [13]; (c) The transformed mesh.s(a) jaw down/up (b) jaw back/front (c) lips spread/round (d) upper lip up (e) lip corners up Figure 2: Elementary speech movements extracted from statistical analysis of mo</context></contexts></citation><citation id="29166"><authors>F H Pighin,R Szeliski,D Salesin</authors><title>Resynthesizing facial animation through 3D modelbased tracking</title><venue>International Conference on Computer Vision</venue><venType>CONFERENCE</venType><year>1999</year><pages>143--150</pages><volume>1</volume><raw>Pighin, F.H., Szeliski, R., and Salesin, D. (1999) Resynthesizing facial animation through 3D modelbased tracking. International Conference on Computer Vision, 1: p. 143-150.</raw><contexts><context>ker variability of articulation, there is also a clear technological need for generic models that can be adapted to speaker-specific anatomy and movements: systems such as model-based computer vision [5, 14] or MPEG-4/SNHC coding scheme [12] require a generic mesh to be adapted via separated conformation and animation parameters to a real speaker. This paper describes an approach for building shape model</context><context>peaker-specific raw motion capture data. An extension of this approach to appearance models is also sketched. 2 SPEAKER-SPECIFIC TALKING HEADS When using video rewriting [2, 7] or 3D animation models [8, 14], all systems use a speaker-specific shape that computes the displacement of key facial fleshpoints. Motion capture devices (e.g. Qualisys, Vicon) deliver in real-time and with a extreme precision the</context><context> TO SPEAKERSPECIFIC DATA The deformation of a high definition 3D surface towards a set of low definition 3D data is achieved by an original 3D-to-3D matching algorithm.. The generic 3D mesh used here [14] has 5826 vertices connected by 11370 triangles (see Figure 1.b). The 3D articulatory model of the female speaker used here drives 304 fleshpoints : 245 beads for the face as in Figure 1.a, 30 control</context></contexts></citation><citation id="29167"><authors>L Revéret,G Bailly,P Badin</authors><title>MOTHER: a new generation of talking heads providing a flexible articulatory control for videorealistic speech animation</title><venue>in International Conference on Speech and Language Processing. Beijing</venue><venType>CONFERENCE</venType><year>2000</year><pages>755--758</pages><pubAddress>China</pubAddress><raw>Revéret, L., Bailly, G., and Badin, P. (2000) MOTHER: a new generation of talking heads providing a flexible articulatory control for videorealistic speech animation. in International Conference on Speech and Language Processing. Beijing - China. p. 755-758.</raw></citation><citation id="29168"><authors>S Runeson,G Frykholm</authors><title>Visual perception of lifted weight</title><venue>Journal of Experimental Psychology: Human Perception and Performance</venue><venType>JOURNAL</venType><year>1981</year><pages>733--740</pages><volume>7</volume><raw>Runeson, S. and Frykholm, G. (1981) Visual perception of lifted weight. Journal of Experimental Psychology: Human Perception and Performance, 7: p. 733-740.</raw><contexts><context>ometimes subtle and distributed all over the face, but should not be neglected since interlocutors should be quite sensitive to laws governing biological motion (e.g. the experiments of Runeson et al [16, 17] with body movements when carrying imaginary versus real loads). Although its crude linear assumptions do not take into account, for now, saturation due to tissue compression, this multilinear techniq</context></contexts></citation><citation id="29169"><authors>S Runeson,G Frykholm</authors><title>Kinematic specification of dynamics as an informational basis for person and action perception: Expectation, gender recognition, and deceptive intention</title><venue>Journal of Experimental Psychology: General</venue><venType>JOURNAL</venType><year>1983</year><pages>585--615</pages><volume>112</volume><raw>Runeson, S. and Frykholm, G. (1983) Kinematic specification of dynamics as an informational basis for person and action perception: Expectation, gender recognition, and deceptive intention. Journal of Experimental Psychology: General, 112: p. 585-615.</raw><contexts><context>ometimes subtle and distributed all over the face, but should not be neglected since interlocutors should be quite sensitive to laws governing biological motion (e.g. the experiments of Runeson et al [16, 17] with body movements when carrying imaginary versus real loads). Although its crude linear assumptions do not take into account, for now, saturation due to tissue compression, this multilinear techniq</context></contexts></citation><citation id="29170"><authors>R Szeliski,S Lavallée</authors><title>Matching 3-D Anatomical Surfaces with Non-Rigid Deformations using Octree-Splines</title><venue>International Journal of Computer Vision</venue><venType>JOURNAL</venType><year>1996</year><pages>171--186</pages><volume>18</volume><raw>Szeliski, R. and Lavallée, S. (1996) Matching 3-D Anatomical Surfaces with Non-Rigid Deformations using Octree-Splines. International Journal of Computer Vision, 18(2): p. 171-186.</raw><contexts><context>), the second deals with point-to-point distance: a set of 3D fleshpoints {qj} are identified and paired with {rj} vertices of S. The minimization is performed using the Levenberg-Marquardt algorithm [18]. 3.2 Matching a neutral configuration The algorithm described above is applied to the articulatory configuration that provides the same neutral articulation as the static generic model. A minimal set</context></contexts></citation><citation id="29171"><authors>A M Tekalp,J Ostermann</authors><title>Face and 2-D Mesh animation</title><venue>in MPEG-4. Signal Processing: Image Communication</venue><venType>CONFERENCE</venType><year>2000</year><pages>387--421</pages><volume>15</volume><raw>Tekalp, A.M. and Ostermann, J. (2000) Face and 2-D Mesh animation in MPEG-4. Signal Processing: Image Communication, 15: p. 387-421.</raw></citation><citation id="29172"><authors>F Vignoli,C Braccini</authors><title>A text-speech synchronization technique with applications to talking heads</title><venue>in Auditory-Visual Speech Processing Conference</venue><venType>CONFERENCE</venType><year>1999</year><pages>128--132</pages><pubAddress>Santa Cruz, California, USA</pubAddress><raw>Vignoli, F. and Braccini, C. (1999) A text-speech synchronization technique with applications to talking heads. in Auditory-Visual Speech Processing Conference. Santa Cruz, California, USA. p. 128-132.</raw><contexts><context>ess speaker-dependent than articulatory parameters. On the contrary articulatory models are often used to specify how constrictions sizes and positions are reached by speaker-specific speech segments [20]. The current proposal gives access to speaker-specific articulatory models of facial deformations. With reference to a generic face these models describe the speaker-specific consequences of six univ</context></contexts></citation></citations><fileInfo><url>http://www.icp.inpg.fr/ICP/publis/synthese/_mb/clonegen_mb_ISSP03.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>c5b57248688f853ee9a3cc8aae327fbbae6e2c27</sha1></checkSum></checkSums></fileInfo></document>