<document id="10.1.1.1.1568"><title src="SVM HeaderParse 0.1">Learning Partially Observable Action Models</title><abstract src="SVM HeaderParse 0.1">Abstract. In this paper we present tractable algorithms for learning a logical model of actions ’ effects and preconditions in deterministic partially observable domains. These algorithms update a representation of the set of possible action models after every observation and action execution. We show that when actions are known to have no conditional effects, then the set of possible action models can be represented compactly indefinitely. We also show that certain desirable properties hold for actions that have conditional effects, and that sometimes those can be learned efficiently as well. Our approach takes time and space that are polynomial in the number of domain features, and it is the first exact solution that is tractable for a wide class of problems. It does so by representing the set of possible action models using propositional logic, while avoiding general-purpose logical inference. Learning in partially observable domains is difficult and intractable in general, but our results show that it can be solved exactly in large domains in which one can assume some structure for actions ’ effects and preconditions. These results are relevant for more general settings, such as learning HMMs, reinforcement learning, and learning in partially observable stochastic domains. 1</abstract><keywords></keywords><authors></authors><citations src="ParsCit 1.0"><citation id="29520"><authors>Eyal Amir,Stuart Russell</authors><title>Logical filtering</title><venue>in Proc. Eighteenth International Joint Conference on Artificial Intelligence (IJCAI ’03</venue><venType>CONFERENCE</venType><year>2003</year><pages>75--82</pages><publisher>Morgan Kaufmann</publisher><raw>Eyal Amir and Stuart Russell, ‘Logical filtering’, in Proc. Eighteenth International Joint Conference on Artificial Intelligence (IJCAI ’03), pp. 75–82. Morgan Kaufmann, (2003).</raw><contexts><context>rtially observable domains. They are also first to find an action model at the same time that they determine the agent’s knowledge about the state of the world. They draw on intuitions and results of [1] for known (nondeterministic) action models. If we assume that our transition model is fully known, then our results reduce to those of [1] for deterministic actions.sA wide range of virtual domains s</context><context>isfying one of the tuples in � sw-on causes E, sw-on keeps E � � sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state) [1, 20, 10] when the transition model is fully specified. Theorem 3 Let ρ = σ × {R}, where σ ⊆ S and R ⊆ S × A × S, and let 〈ai, oi〉 i≤t be a sequence of actions and observations. If FilterR[〈ai, oi〉 i≤t ](σ) is</context><context> previous section illustrates how the explicit representation of transition belief states may be doubly exponential in the number of domain features and the number 1 Filtering semantics as defined in [1]. � of actions. In this section we follow the intuition that propositional logic can serve to represents ρ more compactly. From here forth we assume that our actions are deterministic. In the followin</context></contexts></citation><citation id="29521"><authors>Scott Benson</authors><title>Inductive learning of reactive action models</title><venue>in Proceedings of the 12th International Conference on Machine Learning (ICML-94</venue><venType>CONFERENCE</venType><year>1995</year><raw>Scott Benson, ‘Inductive learning of reactive action models’, in Proceedings of the 12th International Conference on Machine Learning (ICML-94), (1995).</raw><contexts><context>ic in a version space of STRIPS operators. [15] uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables. [2] presents an approach that is based on inductive logic programming. Most recently, [16] showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at e</context></contexts></citation><citation id="29522"><authors>Lonnie Chrisman</authors><title>Abstract probabilistic modeling of action</title><venue>in Proc. National Conference on Artificial Intelligence (AAAI ’92</venue><venType>CONFERENCE</venType><year>1992</year><publisher>AAAI Press</publisher><raw>Lonnie Chrisman, ‘Abstract probabilistic modeling of action’, in Proc. National Conference on Artificial Intelligence (AAAI ’92). AAAI Press, (1992).</raw><contexts><context>yal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) [3, 12, 13]. It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows </context></contexts></citation><citation id="29523"><authors>Richard Fikes,Peter Hart,Nils Nilsson</authors><title>Learning and executing generalized robot plans</title><venue>Artificial Intelligence</venue><venType>JOURNAL</venType><year>1972</year><pages>251--288</pages><volume>3</volume><raw>Richard Fikes, Peter Hart, and Nils Nilsson, ‘Learning and executing generalized robot plans’, Artificial Intelligence, 3, 251–288, (1972).</raw><contexts><context>sentence (conjunction of literals), a an action, ϕ a fluent. 1. Return Cn L t+1 (ϕt ∧ at ∧ Teff(a, t)). Figure 3. SLAF using distribution over ∧, ∨ 4.1 Always-Executable STRIPS Actions STRIPS actions [4] are deterministic and unconditional (but sometimes not executable). In this section we examine them with the assumption that our they always executable. We return to inexecutability in Section 4.2. L</context></contexts></citation><citation id="29524"><authors>Yolanda Gil</authors><title>Learning by experimentation: Incremental refinement of incomplete planning domains</title><venue>in Proceedings of the 11th International Conference on Machine Learning (ICML-94</venue><venType>CONFERENCE</venType><year>1994</year><pages>10--13</pages><raw>Yolanda Gil, ‘Learning by experimentation: Incremental refinement of incomplete planning domains’, in Proceedings of the 11th International Conference on Machine Learning (ICML-94), pp. 10–13, (1994).</raw><contexts><context>testing our algorithms in large domains, including over 1000 features (see [6] for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains. [5, 19] learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators. [15] uses a general-purpose classification system (in their case, MSDD) to l</context></contexts></citation><citation id="29525"><authors>Brian</authors><title>Hlubocky and Eyal Amir, ‘Knowledge-gathering agents in adventure games</title><venue>in AAAI-04 Workshop on Challenges in Game AI</venue><venType>CONFERENCE</venType><year>2004</year><publisher>AAAI Press</publisher><raw>Brian Hlubocky and Eyal Amir, ‘Knowledge-gathering agents in adventure games’, in AAAI-04 Workshop on Challenges in Game AI. AAAI Press, (2004).</raw><contexts><context>sA wide range of virtual domains satisfy our assumptions of determinism and structured actions, and we are in the process of testing our algorithms in large domains, including over 1000 features (see [6] for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains. [5, 19] learn STRIPS actions with parameters by finding the most general and m</context></contexts></citation><citation id="29526"><authors>Leslie Pack Kaelbling,Michael L Littman,Anthony R Cassandra</authors><title>Planning and acting in partially observable stochastic domains</title><venue>Artificial Intelligence</venue><venType>JOURNAL</venType><year>1998</year><pages>99--134</pages><volume>101</volume><raw>Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra, ‘Planning and acting in partially observable stochastic domains’, Artificial Intelligence, 101, 99–134, (1998).</raw><contexts><context>a hill-climbing algorithm which is only guaranteed to reach a local optima, and there is no time guarantee for convergence on this local optima. Reinforcement learning in partially observable domains [7] can be Eyal Amir Computer Science Department University of Illinois, Urbana-Champaign Urbana, IL 61801, USA eyal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving it </context></contexts></citation><citation id="29527"><authors>Leslie Pack Kaelbling,Michael L Littman,Andrew W Moore</authors><title>Reinforcement learning: a survey</title><venue>Journal of Artificial Intelligence Research</venue><venType>JOURNAL</venType><year>1996</year><volume>4</volume><raw>Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore, ‘Reinforcement learning: a survey’, Journal of Artificial Intelligence Research, 4, 237–285, (1996).</raw><contexts><context>ning transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs) [17] and to reinforcement learning [8], both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm, which is a special case of EM. It is a</context></contexts></citation><citation id="29528"><authors>Michael Kearns,Yishay Mansour,Andrew Y Ng</authors><title>Approximate planning in large pomdps via reusable trajectories</title><venue>in Proceedings of the 12th Conference on Neural Information Processing Systems (NIPS’99</venue><venType>CONFERENCE</venType><year>2000</year><pages>1001--1007</pages><publisher>MIT Press</publisher><raw>Michael Kearns, Yishay Mansour, and Andrew Y. Ng, ‘Approximate planning in large pomdps via reusable trajectories’, in Proceedings of the 12th Conference on Neural Information Processing Systems (NIPS’99), pp. 1001–1007. MIT Press, (2000).</raw><contexts><context> this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model [9, 14]. Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states [11], but rein</context></contexts></citation><citation id="29529"><authors>Fangzhen Lin,Ray Reiter</authors><title>How to Progress a Database</title><venue>Artificial Intelligence</venue><venType>JOURNAL</venType><year>1997</year><pages>131--167</pages><volume>92</volume><raw>Fangzhen Lin and Ray Reiter, ‘How to Progress a Database’, Artificial Intelligence, 92(1-2), 131–167, (1997).</raw><contexts><context>isfying one of the tuples in � sw-on causes E, sw-on keeps E � � sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state) [1, 20, 10] when the transition model is fully specified. Theorem 3 Let ρ = σ × {R}, where σ ⊆ S and R ⊆ S × A × S, and let 〈ai, oi〉 i≤t be a sequence of actions and observations. If FilterR[〈ai, oi〉 i≤t ](σ) is</context></contexts></citation><citation id="29530"><authors>Michael L Littman</authors><title>Algorithms for sequential decision making</title><venType>TECHREPORT</venType><year>1996</year><tech>Ph.D. dissertation</tech><raw>Michael L. Littman, Algorithms for sequential decision making, Ph.D. dissertation, Department of Computer Science, Brown University, 1996. Technical report CS-96-09.</raw><contexts><context> model [9, 14]. Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states [11], but reinforcement learning has no similar solution known to us. In this paper we present a formal, exact, many times tractable, solution to the problem of simultaneously learning and filtering (SLAF</context></contexts></citation><citation id="29531"><authors>R Andrew McCallum</authors><title>Instance-based utile distinctions for reinforcement learning with hidden state</title><venue>in Proceedings of the 12th International Conference on Machine Learning (ICML-95</venue><venType>CONFERENCE</venType><year>1995</year><publisher>Morgan Kaufmann</publisher><raw>R. Andrew McCallum, ‘Instance-based utile distinctions for reinforcement learning with hidden state’, in Proceedings of the 12th International Conference on Machine Learning (ICML-95). Morgan Kaufmann, (1995).</raw><contexts><context>yal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) [3, 12, 13]. It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows </context></contexts></citation><citation id="29532"><authors>Nicolas Meuleau,Leonid Peshkin,Kee-Eung Kim,Leslie Pack</authors><title>Kaelbling, ‘Learning finite-state controllers for partially observable environments</title><venue>in Proc. Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI ’99</venue><venType>CONFERENCE</venType><year>1999</year><publisher>Morgan Kaufmann</publisher><raw>Nicolas Meuleau, Leonid Peshkin, Kee-Eung Kim, and Leslie Pack Kaelbling, ‘Learning finite-state controllers for partially observable environments’, in Proc. Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI ’99). Morgan Kaufmann, (1999).</raw><contexts><context>yal@cs.uiuc.edu solved (approximately) by interleaving learning the POMDP with solving it (the learning and solving are both approximate because finite memory or finite granularity is always assumed) [3, 12, 13]. It is important to notice that this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows </context></contexts></citation><citation id="29533"><authors>Andrew Y Ng,Michael Jordan</authors><title>Pegasus: A policy search method for large mdps and pomdps</title><venue>in Proc. Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI ’00</venue><venType>CONFERENCE</venType><year>2000</year><pages>406--415</pages><publisher>Morgan Kaufmann</publisher><raw>Andrew Y. Ng and Michael Jordan, ‘Pegasus: A policy search method for large mdps and pomdps’, in Proc. Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI ’00), pp. 406–415. Morgan Kaufmann, (2000).</raw><contexts><context> this problem is harder than solving POMDPs. In some cases, one can solve the POMDP with some guarantee for relatively fast convergence and approximation, if one knows the underlying transition model [9, 14]. Also, in deterministic cases, computing the optimal undiscounted infinite horizon policy in (known) POMDPs is PSPACE-hard (NP-complete if a polynomial horizon) in the number of states [11], but rein</context></contexts></citation><citation id="29534"><authors>Tim Oates,Paul R Cohen</authors><title>Searching for planning operators with context-dependent and probabilistic effects</title><venue>in Proc. National Conference on Artificial Intelligence (AAAI ’96</venue><venType>CONFERENCE</venType><year>1996</year><pages>863--868</pages><publisher>AAAI Press</publisher><raw>Tim Oates and Paul R. Cohen, ‘Searching for planning operators with context-dependent and probabilistic effects’, in Proc. National Conference on Artificial Intelligence (AAAI ’96), pp. 863–868. AAAI Press, (1996).</raw><contexts><context>ion’s effects and preconditions focused on fully observable domains. [5, 19] learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators. [15] uses a general-purpose classification system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables. [2] presents an approach that is based on induc</context></contexts></citation><citation id="29535"><authors>Hanna M Pasula,Luke S Zettlemoyer,Leslie Pack</authors><title>Kaelbling, ‘Learning probabilistic relational planning rules</title><year>2004</year><publisher>AAAI Press</publisher><raw>Hanna M. Pasula, Luke S. Zettlemoyer, and Leslie Pack Kaelbling, ‘Learning probabilistic relational planning rules’. AAAI Press, (2004).</raw><contexts><context>system (in their case, MSDD) to learn the effects and preconditions of actions, identifying irrelevant variables. [2] presents an approach that is based on inductive logic programming. Most recently, [16] showed how to learn stochastic actions with no conditional effects (i.e., the same stochastic change occurs at every state in which the action is executable). The common theme among these approaches </context></contexts></citation><citation id="29536"><authors>L R Rabiner</authors><title>A tutorial on hidden Markov models and selected applications in speech recognition</title><venue>Proceedings of the IEEE</venue><venType>CONFERENCE</venType><year>1989</year><volume>77</volume><raw>L. R. Rabiner, ‘A tutorial on hidden Markov models and selected applications in speech recognition’, Proceedings of the IEEE, 77(2), 257–285, (February 1989).</raw><contexts><context>change to flipping the switch. Learning transition models in partially observable domains is hard. In stochastic domains, learning transition models is central to learning Hidden Markov Models (HMMs) [17] and to reinforcement learning [8], both of which afford only solutions that are not guaranteed to approximate the optimal. In HMMs the transition model is learned using the Baum-Welch algorithm, whic</context></contexts></citation><citation id="29537"><authors>Matthew D Schmill,Tim Oates,Paul R Cohen</authors><title>Learning planning operators in real-world, partially observable environments</title><venue>in Proceedings of the 5th Int’l Conf. on AI Planning and Scheduling (AIPS’00</venue><venType>CONFERENCE</venType><year>2000</year><pages>246--253</pages><publisher>AAAI Press</publisher><raw>Matthew D. Schmill, Tim Oates, and Paul R. Cohen, ‘Learning planning operators in real-world, partially observable environments’, in Proceedings of the 5th Int’l Conf. on AI Planning and Scheduling (AIPS’00), pp. 246–253. AAAI Press, (2000).</raw><contexts><context>hastic change occurs at every state in which the action is executable). The common theme among these approaches is their assumption that the state of the world is fully observed at any point in time. [18] is the only work that considers partial observability, and it does so by assuming that the world is fully observable, giving approximate computation in relevant domains. 2 Filtering Transition Relati</context></contexts></citation><citation id="29538"><authors>Xuemei Wang</authors><title>Learning by observation and practice: an incremental approach for planning operator acquisition</title><venue>in Proceedings of the 12th International Conference on Machine Learning (ICML-95</venue><venType>CONFERENCE</venType><year>1995</year><pages>549--557</pages><publisher>Morgan Kaufmann</publisher><raw>Xuemei Wang, ‘Learning by observation and practice: an incremental approach for planning operator acquisition’, in Proceedings of the 12th International Conference on Machine Learning (ICML-95), pp. 549–557. Morgan Kaufmann, (1995).</raw><contexts><context>testing our algorithms in large domains, including over 1000 features (see [6] for current progress). Previous work on learning action’s effects and preconditions focused on fully observable domains. [5, 19] learn STRIPS actions with parameters by finding the most general and most specific in a version space of STRIPS operators. [15] uses a general-purpose classification system (in their case, MSDD) to l</context></contexts></citation><citation id="29539"><authors>Mary-Anne Winslett</authors><title>Updating Logical Databases</title><year>1990</year><publisher>University Press</publisher><pubAddress>Cambridge</pubAddress><raw>Mary-Anne Winslett, Updating Logical Databases, Cambridge University Press, 1990.</raw><contexts><context>e head and G is the tail of those rules. When G ≡ TRUE we write “a causes F ”. The semantics of a domain description that we choose is compatible with the standard semantics belief update operator of [20]. We define it below by first completing the description and then mapping the completed description to a transition relation. For domain description D we define a transition system with PD and AD the </context><context>isfying one of the tuples in � sw-on causes E, sw-on keeps E � � sw-on causes lit ×{ sw-on causes sw }× sw-on causes ¬lit sw-on keeps lit SLAF reduces to filtering (updating the agent’s belief state) [1, 20, 10] when the transition model is fully specified. Theorem 3 Let ρ = σ × {R}, where σ ⊆ S and R ⊆ S × A × S, and let 〈ai, oi〉 i≤t be a sequence of actions and observations. If FilterR[〈ai, oi〉 i≤t ](σ) is</context></contexts></citation></citations><fileInfo><url>http://www.cs.uiuc.edu/~eyal/papers/model-learning.0.35-cogrob04-final.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>c252496bd3a246a16581b8756dbb651b841b0cd7</sha1></checkSum></checkSums></fileInfo></document>