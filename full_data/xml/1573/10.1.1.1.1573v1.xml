<document id="10.1.1.1.1573"><clusterid>1509</clusterid><title src="INFERENCE">Stopping criterion for boosting-based data reduction techniques: from binary to multiclass problems</title><abstract src="user correction - Legacy Corrections">So far, boosting has been used to improve the quality of moderately accurate learning algorithms,  by weighting and combining many of their weak hypotheses into a final classifier with theoretically  high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt  boosting properties to data reduction techniques. In this particular context, the objective was not  only to improve the success rate, but also to reduce the time and space complexities due to the  storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In  that framework, each weak hypothesis, which is usually built and weighted from the learning set,  is replaced by a single learning instance. The weight given by boosting defines in that case the  relevance of the instance, and a statistical test allows one to decide whether it can be discarded  without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed  problems with two classes. It is the aim of the present paper to relax the class constraint, and  extend our contribution to multiclass problems. Beyond data reduction, experimental results are  also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting  rule brings to weighted nearest neighbor classifiers.</abstract><year src="INFERENCE">2002</year><venue src="INFERENCE">Journal of Machine Learning Research</venue><venType src="INFERENCE">JOURNAL</venType><pages src="INFERENCE">863--885</pages><volume src="INFERENCE">3</volume><keywords></keywords><authors><author id="2405933"><clusterid>0</clusterid><name src="SVM HeaderParse 0.1">Marc Sebban</name><affil src="SVM HeaderParse 0.2">Eurise, Faculty of Sciences; University of Jean Monnet</affil><address src="SVM HeaderParse 0.1">42023 Saint-Etienne Cedex 2, France</address><email src="SVM HeaderParse 0.1">MARC.SEBBAN@UNIV-ST-ETIENNE.FR</email><order src="null">1</order></author><author id="2405934"><clusterid>0</clusterid><name src="SVM HeaderParse 0.1">Richard Nock</name><affil src="SVM HeaderParse 0.2">GRIMAAG, Scientific Department; French West</affil><address src="SVM HeaderParse 0.1">Indies and Guiana University; 97275 Schoelcher Cedex, Martinique, France</address><email src="SVM HeaderParse 0.1">RNOCK@MARTINIQUE.UNIV-AG.FR</email><order src="null">2</order></author><author id="2405935"><clusterid>0</clusterid><name src="SVM HeaderParse 0.1">Stéphane Lallich</name><affil src="SVM HeaderParse 0.2">ERIC, Department of Economics</affil><address src="SVM HeaderParse 0.1">University of Lyon 2, 69676 Bron Cedex, France</address><email src="SVM HeaderParse 0.1">LALLICH@UNIV-LYON2.FR</email><order src="null">3</order></author><author id="2405936"><clusterid>0</clusterid><name src="SVM HeaderParse 0.1">E. Brodley</name><order src="null">4</order></author><author id="2405937"><clusterid>0</clusterid><name src="SVM HeaderParse 0.1">Andrea Danyluk</name><order src="null">5</order></author></authors><citations src="ParsCit 1.0"><citation id="29584"><clusterid>1510</clusterid><authors>Erin Allwein,E Robert</authors><title>Schapire and Yoram Singer. Reducing multiclass to binary: a unifying approach for margin classifiers</title><venue>Journal of Machine Learning Research</venue><venType>JOURNAL</venType><pages>200--0</pages><volume>1</volume><raw>Erin Allwein, Robert E. Schapire and Yoram Singer. Reducing multiclass to binary: a unifying approach for margin classifiers. Journal of Machine Learning Research, 1, 113–141, 2000.</raw><paperid>10.1.1.1.1573</paperid></citation></citations><fileInfo><crawldate>Nov 19, 2007</crawldate><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><urls><url>http://www.univ-ag.fr/~rnock/Articles/Drafts/jmlr02-snl.pdf</url></urls></fileInfo></document>