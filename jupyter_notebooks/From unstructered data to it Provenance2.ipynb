{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From unstructered data to it Provenance\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> \n",
    "<h3>EScience - PPG IC/UFF 2017.1</h3>\n",
    "\n",
    "<h4>Daniel Junior, Kid Valeriano</h4>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relembrando...\n",
    "O projeto tem como base o artigo <strong>Provenance as data mining: Combining file system metadata with content analysis</strong>.\n",
    "</hr>\n",
    "<li>A ideia é utilizar técnicas de Mineração de Dados para reconstruir a proveniência de dados não-estruturados.</li>\n",
    "<li>Os dados não-estruturados utilizados como dados de entrada para o projeto são arquivos textos contendo versões de um artigo até que o mesmo chegasse a sua edição final.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arquitetura\n",
    "<div width=\"100%\" height=\"100%\">\n",
    "    <img src=\"images/arquitetura.png\" height=\"400\" width=\"800\" style=\"margin: 0 auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projeto\n",
    "<ul>\n",
    "    <li>Linguagem Python</li>\n",
    "    <li>Biblioteca Sci-kit Learn</li>\n",
    "    <li>Os dados usados no experimento são do repositório <strong><a href=\"https://l.facebook.com/l.php?u=http%3A%2F%2F137.207.234.78%2Fsearch%2Fdetail%2F&h=ATMRd4R-lyI7h1CWdhxvn6UWFuIQ6Sk5lh5mt_Yl5P4cZtgMLg2Eab3JBX_fKeG0-K69PJBwnMqh8BrtHvD__C71t0gmJg-NTkGuLNtxIehOx2CwfwQx_44sEEALKLVSY6y_fg\">CiteSeerX</a></strong></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projeto\n",
    "\n",
    "Os passos a serem seguidos são:\n",
    "\n",
    "<ol>\n",
    "    <li>Dado um diretório que contenha as revisões de arquivos, devo ser capaz de recuperar o conteúdo de cada arquivo.</li>\n",
    "    <li>Representar o conteúdo do arquivo de uma forma que seja possível aplicar algoritmos de Clustering.</li>\n",
    "    <li>Executar o algoritmo para criar os clusters.</li>\n",
    "    <li>Para cada arquivo em um cluster, recuperar os metados relativos ao dados de modificação do arquivo.</li>\n",
    "    <li>Com os metadados em mãos, exportar para o PROV-Model, fazendo o armazenamento em um banco relacional (SQLite).</li>\n",
    "    <li>Ser capaz de receber um arquivo de entrada e retornar uma representação gŕafica da proveniência do mesmo.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preparação de dados\n",
    "Os dados obtiveram-se de um repositório chamado CiteSeerX, a qual tem os dados separados em dois tipos de formatos, todo o documento contido em um arquivo TXT e alguns características como o titulo, resumo, data de publicação, autores e as referências em um arquivo estruturado XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div width=\"100%\" height=\"100%\">\n",
    "    <img src=\"images/txt.png\" height=\"50\" width=\"500\" style=\"margin: 0 auto;\"/>\n",
    "    <img src=\"images/xml.png\" height=\"300\" width=\"700\" style=\"margin: 0 auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 1: Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os, time\n",
    "from stat import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "CLUSTERS_NUMBER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"data\"\n",
    "filenames = []\n",
    "docs = []\n",
    "for path, dirs, files in os.walk(PATH):\n",
    "    for filename in files:\n",
    "        fullpath = os.path.join(path, filename)\n",
    "        with open(fullpath, 'r') as f:\n",
    "            data = f.read()\n",
    "            filenames.append(fullpath)\n",
    "            docs.append(data)        \n",
    "\n",
    "print(filenames[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para realizar o trabalho precisa-se ter vários dados separados em listas com a finalidade de uma fácil análise: título do artigo, conteúdo do texto, referências do artigo, e o tempo nas que se fizeram as publicações.\n",
    "Usando os 2 primeiro atributos para  realizar o agrupamento de documentos similares usando o algoritmo K-Means, e os últimos usados para reconstruir a procedência dos artigos analisados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "titles = open('title_list.txt').read().split('\\n')\n",
    "titles = titles[:100]\n",
    "\n",
    "synopses = open('textBody_list.txt').read().split('\\n')\n",
    "synopses = synopses[:100]\n",
    "\n",
    "links = open('reference_list.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "genres = open('time_list.txt').read().split('\\n')\n",
    "genres = genres[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_data = shuffle(docs, random_state=42)[int(len(docs)*0.2):]\n",
    "test_data = shuffle(docs, random_state=42)[:int(len(docs)*0.2)]\n",
    "\n",
    "train_filenames = shuffle(filenames, random_state=42)[int(len(docs)*0.2):]\n",
    "test_filenames = shuffle(filenames, random_state=42)[:int(len(docs)*0.2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\") #sacar la raiz de una palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que o torna mais eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\") #sacar la raiz de una palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 2: Representação vetorial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = ara\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "print(\"Dimensões da matriz:\")\n",
    "print(tfidf_matrix.todense().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 2: Representação vetorial dos dados\n",
    "\n",
    "<div width=\"100%\" height=\"100%\">\n",
    "    <img src=\"images/tfidf.png\" style=\"margin: 0 auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 3: Executar algoritmo de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=CLUSTERS_NUMBER, init='k-means++', max_iter=100, n_init=1,verbose=False)\n",
    "km.fit(tfidf_matrix.todense())\n",
    "print(km.labels_[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Passo 4: Recuperar os metadados dos arquivos de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clusters_predict = km.predict(tfidf_vectorizer.transform(test_data))\n",
    "cluster_0_idx =  [i for i, x in enumerate(clusters_predict) if x == 0]\n",
    "filenames_cluster_0 = [test_filenames[x] for x in cluster_0_idx ]\n",
    "\n",
    "for filename in filenames_cluster_0[:3]:\n",
    "    st = os.stat(filename)\n",
    "    print(\"filename:\", filename)\n",
    "    print(\"file modified:\", time.asctime(time.localtime(st[ST_CTIME])))\n",
    "    print(\"file size:\", st[ST_SIZE])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: eduard coma yee baseballers organize teams games cs1 sublime lineups\n",
      "Cluster 1: israeline israelis jfk eduard turklerin pera armenianism armeniaxn arabia jamea\n"
     ]
    }
   ],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "    \n",
    "#km.predict(tfidf_vectorizer.transform(test_data.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,600,600i\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Inconsolata\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Titillium+Web:400,400i,700,700i\" rel=\"stylesheet\">\n",
       "<style>\n",
       ".text_cell_render {\n",
       "font-style: regular;\n",
       "font-family: 'Fira Sans', sans-serif;\n",
       "display: block;\n",
       "}\n",
       "/*font-weight: 200;*/\n",
       "/*text-align: left;\n",
       "line-height: 100%;\n",
       "display: block;\n",
       "}*/\n",
       ".text_cell_render h1 {\n",
       "/*font-size: 24pt;*/\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       "\n",
       ".reveal h1 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#4a4a4a;\n",
       "}\n",
       ".text_cell_render h2 {\n",
       "/*font-size: 21pt;*/\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 19pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 17pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 15pt;*/\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.3em;\n",
       "color:#595959;\n",
       "}\n",
       "div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 100%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       ".reveal div.text_cell_render{\n",
       "line-height: 120%;\n",
       "font-size: 74%;\n",
       "font-weight: 400;\n",
       "text-align: justify;\n",
       "margin-left:0em;\n",
       "margin-right:0em;\n",
       "}\n",
       "\n",
       ".reveal h2 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h3 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "/*font-size: 24pt;*/\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal h4 {\n",
       "font-family: 'Titillium Web', sans-serif;\n",
       "font-weight: bold;\n",
       "margin-bottom: 0.1em;\n",
       "margin-top: 0.5em;\n",
       "color:#595959;\n",
       "}\n",
       ".reveal .code_cell {\n",
       "    font-size: 83%;\n",
       "}\n",
       ".reveal code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".reveal pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "code {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       "pre {\n",
       "font-family: 'Inconsolata', monospace;\n",
       "}\n",
       ".CodeMirror{\n",
       "font-family: \"Inconsolata\", monospace;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "from urllib.request import urlopen\n",
    "HTML(urlopen('https://raw.githubusercontent.com/lmarti/jupyter_custom/master/custom.include').read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
