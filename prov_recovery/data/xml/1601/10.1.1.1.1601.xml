<document id="10.1.1.1.1601"><title src="SVM HeaderParse 0.1">classifiers</title><abstract src="SVM HeaderParse 0.1">maximum entropy approach to multiple</abstract><keywords></keywords><authors><author id="4446"><name src="SVM HeaderParse 0.1">Francois Fouss</name><address src="SVM HeaderParse 0.1">B-1348 Louvain-la-Neuve, Belgium</address><email src="SVM HeaderParse 0.1">fouss@isys.ucl.ac.be</email><order>1</order></author><author id="4447"><name src="SVM HeaderParse 0.1">Marco Saerens</name><address src="SVM HeaderParse 0.1">B-1348 Louvain-la-Neuve, Belgium</address><email src="SVM HeaderParse 0.1">saerens@isys.ucl.ac.be</email><order>2</order></author></authors><citations src="ParsCit 1.0"><citation id="30035"><authors>D Chen,X Cheng</authors><title>An asymptotic analysis of some expert fusion methods</title><venue>Pattern Recognition Letters</venue><venType>JOURNAL</venType><pages>200--1</pages><volume>22</volume><raw>D. Chen and X. Cheng. An asymptotic analysis of some expert fusion methods. Pattern Recognition Letters, 22:901–904, 2001.</raw><contexts><context>erent approaches have been developped for experts opinions fusion, including weighted average (see for instance [2], [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see for instance [1], [11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analys</context></contexts></citation><citation id="30036"><authors>R M Cooke</authors><title>Experts in uncertainty</title><year>1991</year><publisher>Oxford University Press</publisher><raw>R. M. Cooke. Experts in uncertainty. Oxford University Press, 1991.</raw><contexts><context>le is illustrated by some simulations. 1. Introduction The fusion of various sources of knowledge has been an active subject of research since more than three decades (for some review references, see [2], [5], [7]). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance [12]). Many different approaches have been developped for experts opinions </context></contexts></citation><citation id="30037"><authors>D Dubois,M Grabisch,H Prade,P Smets</authors><title>Assessing the value of a candidate: Comparing belief function and possibility theories</title><venue>Proceedings of the Fifteenth international conference on Uncertainty in Artificial Intelligence</venue><venType>CONFERENCE</venType><year>1999</year><pages>170--177</pages><raw>D. Dubois, M. Grabisch, H. Prade, and P. Smets. Assessing the value of a candidate: Comparing belief function and possibility theories. Proceedings of the Fifteenth international conference on Uncertainty in Artificial Intelligence, pages 170– 177, 1999.</raw><contexts><context>, [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see for instance [1], [11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc. One of these approaches is based on maximum entropy modeling (see [16], [18]). Maximum entropy is a v</context></contexts></citation><citation id="30038"><authors>S-C Fang,J Rajasekera,H-S J Tsao</authors><title>Entropy optimization and mathematical programming</title><year>1997</year><publisher>Kluwer Academic Publishers</publisher><raw>S.-C. Fang, J. Rajasekera, and H.-S. J. Tsao. Entropy optimization and mathematical programming. Kluwer Academic Publishers, 1997.</raw><contexts><context>lve this problem. Once we have verified that there is indeed a feasible solution, an iterative scaling procedure allowing to estimate the Gsum, Gop, Grel, Gcor can easily be derived (see for instance [4]). The iterative scaling procedure aims to satisfy in turn each constraint, and iterate on the set of constraints (as proposed by many authors, for instance [14]). It has been shown that this iterativ</context><context> l|x) ≤ � P(d(k) = i, d(l) = i|x) ≤ σ + (k, l|x) for k, l = 1 . . .m (3.12) i Once more, numerical procedures related to iterative scaling can be used in order to compute the maximum entropy solution [4]. We have to maximize (3.5) subject to constraints (2.2), (2.3), (3.11), (3.12). 3.5. Some extensions The maximum entropy model presented in Section 2 can be extended in several ways. For instance, if</context></contexts></citation><citation id="30039"><authors>C Genest,J V Zidek</authors><title>Combining probability distributions: A critique and an annotated bibliography</title><venue>Statistical Science</venue><venType>JOURNAL</venType><year>1986</year><volume>36</volume><raw>C. Genest and J. V. Zidek. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, 36:114–148, 1986.</raw><contexts><context> illustrated by some simulations. 1. Introduction The fusion of various sources of knowledge has been an active subject of research since more than three decades (for some review references, see [2], [5], [7]). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance [12]). Many different approaches have been developped for experts opinions fusio</context></contexts></citation><citation id="30040"><authors>A Golan,G Judge,D Miller</authors><title>Maximum entropy econometrics: Robust estimation with limited data</title><year>1996</year><publisher>John Wiley and Sons</publisher><raw>A. Golan, G. Judge, and D. Miller. Maximum entropy econometrics: Robust estimation with limited data. John Wiley and Sons, 1996.</raw><contexts><context>f these experts, etc.sIn this work, we propose a new model of experts opinions integration, based on a maximum entropy model (for a review of maximum entropy theory and applications, see for instance [6], [8], [9] or [10]). In this paper, we use the term “experts opinions”, but it should be clear that we can use exactly the same procedures for “classifiers combination”. In other words, we could subst</context></contexts></citation><citation id="30041"><authors>R A Jacobs</authors><title>Methods for combining experts’ probability assessments</title><venue>Neural Computation</venue><venType>JOURNAL</venType><year>1995</year><volume>7</volume><raw>R. A. Jacobs. Methods for combining experts’ probability assessments. Neural Computation, 7:867–888, 1995.</raw><contexts><context>strated by some simulations. 1. Introduction The fusion of various sources of knowledge has been an active subject of research since more than three decades (for some review references, see [2], [5], [7]). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance [12]). Many different approaches have been developped for experts opinions fusion, in</context></contexts></citation><citation id="30042"><authors>F Jelinek</authors><title>Statistical methods for speech recognition</title><year>1997</year><publisher>The MIT Press</publisher><raw>F. Jelinek. Statistical methods for speech recognition. The MIT Press, 1997.</raw><contexts><context>se experts, etc.sIn this work, we propose a new model of experts opinions integration, based on a maximum entropy model (for a review of maximum entropy theory and applications, see for instance [6], [8], [9] or [10]). In this paper, we use the term “experts opinions”, but it should be clear that we can use exactly the same procedures for “classifiers combination”. In other words, we could substitute</context></contexts></citation><citation id="30043"><authors>J N Kapur,H K Kesavan</authors><title>The generalized maximum entropy principle (with applications</title><year>1987</year><publisher>Sandford Educational Press</publisher><raw>J. N. Kapur and H. K. Kesavan. The generalized maximum entropy principle (with applications). Sandford Educational Press, 1987.</raw><contexts><context>perts, etc.sIn this work, we propose a new model of experts opinions integration, based on a maximum entropy model (for a review of maximum entropy theory and applications, see for instance [6], [8], [9] or [10]). In this paper, we use the term “experts opinions”, but it should be clear that we can use exactly the same procedures for “classifiers combination”. In other words, we could substitute “exp</context></contexts></citation><citation id="30044"><authors>J N Kapur,H K Kesavan</authors><title>Entropy optimization principles with applications</title><year>1992</year><publisher>Academic Press</publisher><raw>J. N. Kapur and H. K. Kesavan. Entropy optimization principles with applications. Academic Press, 1992.</raw><contexts><context>etc.sIn this work, we propose a new model of experts opinions integration, based on a maximum entropy model (for a review of maximum entropy theory and applications, see for instance [6], [8], [9] or [10]). In this paper, we use the term “experts opinions”, but it should be clear that we can use exactly the same procedures for “classifiers combination”. In other words, we could substitute “experts” by</context></contexts></citation><citation id="30045"><authors>J Kittler,F Alkoot</authors><title>Sum versus vote fusion in multiple classifier systems</title><venue>IEEE Transactions on Pattern Analysis and Machine Intelligence</venue><venType>JOURNAL</venType><year>2003</year><pages>11--5</pages><volume>25</volume><raw>J. Kittler and F. Alkoot. Sum versus vote fusion in multiple classifier systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(1):110– 115, 2003.</raw><contexts><context> approaches have been developped for experts opinions fusion, including weighted average (see for instance [2], [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see for instance [1], [11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17</context></contexts></citation><citation id="30046"><authors>J Kittler,M Hatef,R Duin,J Matas</authors><title>On combining classifiers</title><venue>IEEE Transactions on Pattern Analysis and Machine Intelligence</venue><venType>JOURNAL</venType><year>1998</year><volume>20</volume><raw>J. Kittler, M. Hatef, R. Duin, and J. Matas. On combining classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3):226–239, 1998.</raw><contexts><context>research since more than three decades (for some review references, see [2], [5], [7]). It has recently been successfully applied to the problem of classifiers combination or fusion (see for instance [12]). Many different approaches have been developped for experts opinions fusion, including weighted average (see for instance [2], [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see f</context></contexts></citation><citation id="30047"><authors>G J Klir,T A Folger</authors><title>Fuzzy sets, uncertainty, and information</title><year>1988</year><publisher>Prentice-Hall</publisher><raw>G. J. Klir and T. A. Folger. Fuzzy sets, uncertainty, and information. Prentice-Hall, 1988.</raw><contexts><context>e (see for instance [2], [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see for instance [1], [11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc. One of these approaches is based on maximum entropy modeling (see [16], [18]). </context></contexts></citation><citation id="30048"><authors>H Ku,S Kullback</authors><title>Approximating discrete probability distributions</title><venue>IEEE Transactions on Information Theory</venue><venType>JOURNAL</venType><year>1969</year><volume>15</volume><raw>H. Ku and S. Kullback. Approximating discrete probability distributions. IEEE Transactions on Information Theory, 15(4):444–447, 1969.</raw><contexts><context>can easily be derived (see for instance [4]). The iterative scaling procedure aims to satisfy in turn each constraint, and iterate on the set of constraints (as proposed by many authors, for instance [14]). It has been shown that this iterative procedure converges to the solution provided that there exists a feasible solution to the problem (that is, the set of constraints can be satisfied). Indeed, t</context></contexts></citation><citation id="30049"><authors>F Lad</authors><title>Operational subjective statistical methods</title><year>1996</year><publisher>John Wiley and Sons</publisher><raw>F. Lad. Operational subjective statistical methods. John Wiley and Sons, 1996.</raw><contexts><context>aches have been developped for experts opinions fusion, including weighted average (see for instance [2], [7]), Bayesian fusion (see for instance [2], [7]), majority vote (see for instance [1], [11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc</context><context>sonal expectation of the expert, i.e. the proportion of times a given expert k would choose alternative i, when observing x (for a general introduction to the concept of subjective probabilities, see [15]). Our objective is to seek the joint probability density of the event, y = i, as well as the experts opinions, d(k) = ik: P(y = i, d(1) = i1, d(2) = i2, . . . , d(m) = im|x) (2.1) 2sThis joint probab</context></contexts></citation><citation id="30050"><authors>W B Levy,H Delic</authors><title>Maximum entropy aggregation of individual opinions</title><venue>IEEE Transactions on Systems, Man and Cybernetics</venue><venType>JOURNAL</venType><year>1994</year><volume>24</volume><raw>W. B. Levy and H. Delic. Maximum entropy aggregation of individual opinions. IEEE Transactions on Systems, Man and Cybernetics, 24(4):606–613, 1994.</raw><contexts><context>y theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc. One of these approaches is based on maximum entropy modeling (see [16], [18]). Maximum entropy is a versatile modeling technique allowing to easily integrate various constraints, such as correlation between experts, reliability of these experts, etc.sIn this work, we pr</context></contexts></citation><citation id="30051"><authors>C Merz</authors><title>Using correspondence analysis to combine classifiers</title><venue>Machine Learning</venue><venType>CONFERENCE</venType><year>1999</year><volume>36</volume><raw>C. Merz. Using correspondence analysis to combine classifiers. Machine Learning, 36:226–239, 1999.</raw><contexts><context>11], [15]), models coming from incertainty reasoning: fuzzy logic, possibility theory [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc. One of these approaches is based on maximum entropy modeling (see [16], [18]). Maximum entropy is a versatile modeling technique allowing to easily integrate various constraints, such as correl</context></contexts></citation><citation id="30052"><authors>I J Myung,S Ramamoorti,J Andrew D Bailey</authors><title>Maximum entropy aggregation of expert predictions</title><venue>Management Science</venue><venType>JOURNAL</venType><year>1996</year><volume>42</volume><raw>I. J. Myung, S. Ramamoorti, and J. Andrew D. Bailey. Maximum entropy aggregation of expert predictions. Management Science, 42(10):1420–1436, 1996.</raw><contexts><context>ry [13] (see for instance [3]), standard multivariate statistical analysis techniques such as correpondence analysis [17], etc. One of these approaches is based on maximum entropy modeling (see [16], [18]). Maximum entropy is a versatile modeling technique allowing to easily integrate various constraints, such as correlation between experts, reliability of these experts, etc.sIn this work, we propose </context><context>le situations that can be encountered, i.e. by computing the marginal P(y = i|x), where x is the feature vector on which we base our prediction. While the main idea is similar, our model differs from [18] in the formulation of the problem (we focus on quantities that are relevant to classification problems, and can easily be computed for classifiers: success rate, degree of agreement, etc) and in the </context><context>ividual opinions are aggregated. Furthermore, we also tackle the problem of incompatible constraints; that is, when there is no feasible solution to the problem, a situation that is not mentionned by [18]. Section 2 introduces the problem and our notations. Section 3 develops the maximum entropy solution. Section 4 presents some simulations results. Section 5 is the conclusion. 2. Statement of the pro</context><context>appear in the context x; that is, on all the different decisions of the experts, where each situation is weighted by its probability of appearance. Notice that, in a different framework, Myung et al. [18] proposed to compute the a posteriori probability of y conditional on expert’s probability of taking a given decision. This is, however, not well-defined since the experts provide a subjective probabi</context></contexts></citation></citations><fileInfo><url>http://www.isys.ucl.ac.be/staff/francois/Articles/Saerens2004a.pdf</url><repID>rep1</repID><conversionTrace>PDFLib TET</conversionTrace><checkSums><checkSum><fileType>pdf</fileType><sha1>5c44c20a5aa6e6e8e8e696880bd0b3f911a55c8c</sha1></checkSum></checkSums></fileInfo></document>